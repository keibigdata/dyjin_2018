{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import gensim \n",
    "import mglearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import konlpy\n",
    "import ckonlpy\n",
    "from konlpy.utils import pprint\n",
    "from konlpy.tag import Hannanum\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from konlpy.tag import Kkma, Mecab\n",
    "import pyLDAvis.gensim\n",
    "import pyLDAvis.sklearn\n",
    "import collections\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import copy\n",
    "import soynlp\n",
    "from soynlp.utils import DoublespaceLineCorpus\n",
    "from soynlp.noun import LRNounExtractor_v2\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from __future__ import print_function\n",
    "from textrankr import TextRank\n",
    "\n",
    "plt.rc('font', family='NanumBarunGothicOTF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeReportAnalyzer():\n",
    "\n",
    "        def __init__ (self):\n",
    "            self.idx_result_df = pd.DataFrame()\n",
    "\n",
    "            input_file_name = \"./result/indexing.txt\"\n",
    "\n",
    "            self.idx_result_df = pd.read_csv(input_file_name,encoding=\"UTF8\",header=None)\n",
    "            self.filtered_idx_result_df = pd.DataFrame()\n",
    "            self.idx_result_df.columns = ['date','title','content_url_list','file_list']\n",
    "\n",
    "            self.climate_words = ['기후변화','가뭄', '강추위', '결빙', '그린란드빙하', '기후', '남극빙하', '녹색성장', '대설', '라니냐',\n",
    "                                  '무더위', '북극빙하', '사막화', '산성비', '쓰나미', '엘니뇨', '열대야', '열섬', '열파', '온난',\n",
    "                                  '온실가스', '우박', '이산화탄소', '이상고온', '이상기온', '이상저온', '장마', '적설', '집중강우',\n",
    "                                  '집중호우', '침수', '탄소', '태풍', '파랑', '패랑', '폭설', '폭염', '폭우', '풍량', '풍수해', '한파',\n",
    "                                  '해수면', '해일', '혹서', '혹한', '홍수', '황사비', '히말라야빙하']\n",
    "\n",
    "            self.climate_words = \"|\".join(self.climate_words)\n",
    "            self.kkma = Kkma()\n",
    "            self.twitter = ckonlpy.tag.Twitter()\n",
    "            self.twitter2 = konlpy.tag.Twitter()\n",
    "            \n",
    "            self.document_topics = None\n",
    "            self.topic_words = None\n",
    "            self.filtered_sentences = None\n",
    "            self.morph_analyzer = None\n",
    "            self.w2v_model = None\n",
    "            self.keyword_list = None\n",
    "            self.topic_list = None\n",
    "            self.corpus = None\n",
    "            self.nouns  = None\n",
    "            self.temp = None\n",
    "            \n",
    "            # 결과저장 디렉토리 생성\n",
    "            dirname = 'result'\n",
    "            if ((os.path.isdir('./' + dirname + '/')) == False):\n",
    "                os.mkdir('./' + dirname + '/')\n",
    "                \n",
    "            dirname = './analysis_result'\n",
    "            if ((os.path.isdir('./' + dirname + '/')) == False):\n",
    "                os.mkdir('./' + dirname + '/')\n",
    "                \n",
    "            dirname = './lda_result'\n",
    "            if ((os.path.isdir('./' + dirname + '/')) == False):\n",
    "                os.mkdir('./' + dirname + '/')\n",
    "\n",
    "            dirname = './doc_trend_result'\n",
    "            if ((os.path.isdir('./' + dirname + '/')) == False):\n",
    "                os.mkdir('./' + dirname + '/')\n",
    "                \n",
    "            dirname = './keyword_frequency_result'\n",
    "            if ((os.path.isdir('./' + dirname + '/')) == False):\n",
    "                os.mkdir('./' + dirname + '/')\n",
    "            \n",
    "            dirname = './keyword_trend_result'\n",
    "            if ((os.path.isdir('./' + dirname + '/')) == False):\n",
    "                os.mkdir('./' + dirname + '/')\n",
    "                \n",
    "            dirname = './lda_stat'\n",
    "            if ((os.path.isdir('./' + dirname + '/')) == False):\n",
    "                os.mkdir('./' + dirname + '/')    \n",
    "                \n",
    "            dirname = './lda_result'\n",
    "            if ((os.path.isdir('./' + dirname + '/')) == False):\n",
    "                os.mkdir('./' + dirname + '/')\n",
    "                \n",
    "            dirname = './sentence_level_cowords'\n",
    "            if ((os.path.isdir('./' + dirname + '/')) == False):\n",
    "                os.mkdir('./' + dirname + '/')\n",
    "            \n",
    "            dirname = './doc_level_cowords'\n",
    "            if ((os.path.isdir('./' + dirname + '/')) == False):\n",
    "                os.mkdir('./' + dirname + '/')\n",
    "                \n",
    "            dirname = './w2v_result'\n",
    "            if ((os.path.isdir('./' + dirname + '/')) == False):\n",
    "                os.mkdir('./' + dirname + '/')\n",
    "                \n",
    "            dirname = './filtered_sentence_result'\n",
    "            if ((os.path.isdir('./' + dirname + '/')) == False):\n",
    "                os.mkdir('./' + dirname + '/')    \n",
    "                \n",
    "            dirname = './press_statistic_result/'\n",
    "            if ((os.path.isdir('./' + dirname + '/')) == False):\n",
    "                os.mkdir('./' + dirname + '/') \n",
    "            \n",
    "            dirname = './sentence_summary_result'\n",
    "            if ((os.path.isdir('./' + dirname + '/')) == False):\n",
    "                os.mkdir('./' + dirname + '/')  \n",
    "                \n",
    "            # 네이버 추가 전처리    \n",
    "            self.idx_result_df['date'] = list(map(str, self.idx_result_df['date']))\n",
    "            self.idx_result_df['file_list'] = list(map(lambda x: os.path.dirname(input_file_name) + \"/\" + os.path.basename(x), self.idx_result_df['file_list'].tolist()))\n",
    "\n",
    "            self.filtered_idx_result_df =  self.idx_result_df\n",
    "            \n",
    "            # remove duplicate\n",
    "            self.remove_duplicate()\n",
    "            \n",
    "        def remove_duplicate(self):\n",
    "            self.filtered_idx_result_df = self.filtered_idx_result_df.drop_duplicates(subset=['title','date'],keep=\"last\")\n",
    "            self.filtered_idx_result_df = self.filtered_idx_result_df.reset_index(drop=True)\n",
    "        \n",
    "        # 날짜 필터링\n",
    "        def filtering_date(self,start_date = None, end_date = None):\n",
    "            dt_index = pd.date_range(start=start_date, end = end_date)\n",
    "            dt_list = dt_index.strftime(\"%Y-%m-%d\").tolist()\n",
    "            idx = list(map(lambda x: x in dt_list, self.filtered_idx_result_df['date'].tolist()))\n",
    "            self.filtered_idx_result_df = self.filtered_idx_result_df.iloc[idx]\n",
    "            self.filtered_idx_result_df = self.filtered_idx_result_df.reset_index(drop=True)\n",
    "            #print(self.idx_result_df[self.idx_result_df['date'] in dt_list]) \n",
    "                        \n",
    "        # 문서 필터링\n",
    "        # is_del = True : 제거 필터링 / is_del = False : 포함 필터링\n",
    "        def filtering_contents(self,rexp,is_del=False):\n",
    "            file_list =  self.filtered_idx_result_df['file_list'].tolist()\n",
    "            #print(file_list)\n",
    "            fidx = []\n",
    "            didx = []\n",
    "            for i in range(len(file_list)):\n",
    "                f = open(file_list[i], 'r',encoding='utf-8',errors=\"ignore\")\n",
    "                doc = f.read()\n",
    "                f.close()\n",
    "                p = re.compile(rexp)\n",
    "                ridx = p.search(doc)\n",
    "                    \n",
    "                if(ridx == None):\n",
    "                   #print(\"없음\")\n",
    "                    didx.append(i)\n",
    "                \n",
    "                else:\n",
    "                    fidx.append(i)\n",
    "                \n",
    "            #print(fidx)\n",
    "            if is_del == False:\n",
    "                #rint(fidx)\n",
    "                self.filtered_idx_result_df = self.filtered_idx_result_df.iloc[fidx]\n",
    "                self.filtered_idx_result_df = self.filtered_idx_result_df.reset_index(drop=True)\n",
    "            else :\n",
    "                self.filtered_idx_result_df = self.filtered_idx_result_df.iloc[didx]\n",
    "                self.filtered_idx_result_df = self.filtered_idx_result_df.reset_index(drop=True)\n",
    "            #print(self.idx_result_df)\n",
    "        \n",
    "\n",
    "        # 필터링 된 결과 저장 \n",
    "        def save_filtered_result(self):\n",
    "            self.idx_result_df.to_csv(\"./result/findexing.txt\",header=None,index=None,encoding=\"UTF8\")\n",
    "            \n",
    "        # 두개의 인덱스 파일 결합   \n",
    "        def combine_idx_result(self, prev):\n",
    "            #print(\"Current\")\n",
    "            #print(self.filtered_idx_result_df)\n",
    "            #print(\"Prev Index result\")\n",
    "            #print(prev.filtered_idx_result_df)\n",
    "            self.filtered_idx_result_df = pd.concat([self.filtered_idx_result_df,prev.filtered_idx_result_df])\n",
    "            self.filtered_idx_result_df = self.filtered_idx_result_df.drop_duplicates(['date','title'])\n",
    "            self.filtered_idx_result_df = self.filtered_idx_result_df.sort_values([\"file_list\"], ascending=True)\n",
    "            self.filtered_idx_result_df = self.filtered_idx_result_df.reset_index(drop=True)\n",
    "            \n",
    "        # 문서 트랜드 분석\n",
    "        def doc_trend_analysis(self, norm=False, prefix=\"\",title=\"환경부 보도자료\",xlab=\"년도\",ylab=\"문서수\"):\n",
    "            \n",
    "            # 날짜 추출\n",
    "            dt_list = self.filtered_idx_result_df['date'].tolist()\n",
    "            dt_cnt = collections.Counter(list(map(lambda x : x.split(\"-\")[0], dt_list)))\n",
    "         \n",
    "            x = list(dt_cnt.keys())\n",
    "            y = list(dt_cnt.values())\n",
    "            \n",
    "            # norm= True (정규화, 상대적 비교)\n",
    "            if(norm == True) :\n",
    "                prefix = prefix + \"norm_\"\n",
    "                dt_index = pd.date_range(start=min(dt_list), end = max(dt_list))\n",
    "                all_dt_list = dt_index.strftime(\"%Y-%m-%d\").tolist()\n",
    "                idx = list(map(lambda x: x in all_dt_list, self.idx_result_df['date'].tolist()))\n",
    "                temp_df = self.idx_result_df.iloc[idx]\n",
    "                temp_df = temp_df.reset_index(drop=True)\n",
    "                temp_df_list = temp_df['date'].tolist()\n",
    "                #print(temp_df)\n",
    "\n",
    "                all_dt_cnt = collections.Counter(list(map(lambda x : x.split(\"-\")[0], temp_df_list)))\n",
    "                all_y = list(all_dt_cnt.values())\n",
    "\n",
    "                y =  np.array(y) / np.array(all_y)\n",
    "                y =  y.tolist()\n",
    "\n",
    "            plt.xticks(rotation=50)\n",
    "            plt.plot(x,y,c=\"b\", lw=2, ls=\"--\", marker=\"o\", ms=10, mec=\"g\", mew=2, mfc=\"r\")\n",
    "            plt.title(title)\n",
    "            plt.xlabel(xlab)\n",
    "            plt.ylabel(ylab)\n",
    "            plt.savefig(\"./doc_trend_result/\" + prefix + \"doc_trends.png\",dpi=200)\n",
    "\n",
    "            result = pd.DataFrame([x,y]).transpose()\n",
    "            result.to_csv(\"./doc_trend_result/\" + prefix + \"doc_trends.csv\",encoding=\"UTF8\")\n",
    "            \n",
    "        # 키워드 빈도수 체크\n",
    "        def keyword_frequency_analysis(self,num=100,is_tfidf = True,is_noun=True, ma=\"twitter2\",prefix=\"\",s_words=[]):\n",
    "            \n",
    "            if ma == \"twitter\":\n",
    "                self.morph_analyzer = self.twitter\n",
    "            if ma == \"twitter2\":\n",
    "                self.morph_analyzer = self.twitter2    \n",
    "                \n",
    "            file_list = self.filtered_idx_result_df['file_list'].tolist()\n",
    "            print(len(file_list))\n",
    "            print(\"Keyword Analysis\")\n",
    "            corpus = [\"\"] * len(file_list)\n",
    "      \n",
    "            # 파일 내용 읽어서 코퍼스에 저장\n",
    "\n",
    "            for i in range(len(file_list)):\n",
    "                #-print(i)\n",
    "                f = open(file_list[i], 'r',encoding='utf-8',errors=\"ignore\")\n",
    "                doc = f.read()\n",
    "                f.close()\n",
    "                corpus[i] = \" \".join(self.morph_analyzer.nouns(doc))\n",
    "\n",
    "\n",
    "            # Stop words \n",
    "            stop_words = [\"전략\", \"연구\", \"평가\", \"마련\", \"조사\", \"관리\", \"보다\", \"분석\", \"구축\",\"개발\",\"정책\",\"대책\",\n",
    "                          \"관련\",\"전문가\",\"예정\",\"이번\",\"환경부\", \"환경\", \"기술\", \"사업\", \"산업\", \"기업\",\"우리나라\", \"또한\",\n",
    "                          \"총회\", \"대한\", \"통해\", \"한국\",\"국내\",\"개최\",\"행사\",\"우수\",\"서울\",\"장관\",\"호남권\",\"피해\",\"해외\",\"일자리\",\n",
    "                          \"박람회\",\"지원\", \"위해\", \"총회\",\"국제\",\"분야\",\"환경기술\",\"10\",\"20\", \"지정\",\"전국\",\"선정\",\"사용\",\"다양\",\"하게\",\n",
    "                          \"대상\",\"대상으로\",\"제품\", \"공단\",\"이상\",\"정보\",\"대해\",\"국민\",\"지난\",\"올해\",\"현재\",\"1부\",\"관한\",\"이용\",\"증가\",\n",
    "                          \"협력\",\"도입\",\"향후\",\"발표\",\"이라\",\"참여\",\"포함\",\"수도권\",\"된다\", \"있으며\",\"1부\",\"등이\",\"이후\",\"15\",\"12\",\n",
    "                          \"정부\", \"회의\",\"기여\",\"활성화\",\"수준\" \"진행\",\"처리\",\"내용\",\"나타났다\", \"대비\",\"하였다\",\"2010년\",\"2011년\",\n",
    "                          \"2012년\",\"2013년\",\"2014년\",\"2015년\",\"2016년\",\"2017년\",\"실천\",\"말했다\",\"현장\",\"구성\",\"최초\",\"확산\",\"홈페이지\",\n",
    "                          \"가능\",\"사진\",\"나타\",\"설명\",\"6월\",\"제공\",\"효과\",\"우리\",\"kr\",\"10월\",\"계기\", \"거나\",\"평균\",\"이하\",\"예방\",\"기관\",\n",
    "                          \"수행\",\"5월\",\"사례\",\"업체\",\"사장\",\"대해서\",\"최근\",\"원장\",\"추가\",\"9월\",\"노력\",\"기존\",\"했다\",\"직접\",\"새로\",\n",
    "                          \"참여하\",\"소개\",\"7월\",\"4월\",\"시작\",\"윤성규\",\"이에\",\"4월\",\"통한\",\"성과\",\"관심\",\"가장\", \"적용\",\"하고\",\"과정\",\n",
    "                          \"보급\",\"동안\",\"연간\",\"거쳐\",\"사업장\",\"주요\",\"시행\",\"프로그램\",\"시설\",\"국민\",\"국립환경과학원\",\"국립생물자원관\",\n",
    "                          \"국립공원관리공단\",\"한펀\",\"논의\",\"공모전\",\"자료\",\"발견\",\"서식\",\"현황\",\"국가\",\"기대\",\"대하\", \"활동\",\"확보\",\"세계\",\n",
    "                          \"국립생태원\",\"홍보\",\"수준\",\"진행\",\"체결\",\"개정\",\"수립\",\"주민\",\"필요\",\"한편\",\"13\",\"5개\",\"11\",\"지역\",\n",
    "                          \"11월\",\"고려\",\"질의응답\",\"서비스\",\"같은\",\"적극\",\"하도\",\"이나\",\"한다\",\"17\",\"16\",\"운영\",\"확대\",\"강화\",\"활용\",\"확인\",\n",
    "                          \"계획\",\"설치\",\"발생\",\"개선\",\"기준\",\"제도\",\"공동\"]\n",
    "            \n",
    "            stop_words = list(set(stop_words))\n",
    "         \n",
    "            if(len(s_words) != 0):\n",
    "                stop_words = s_words\n",
    "                \n",
    "            vect = CountVectorizer(stop_words = stop_words,ngram_range=(1, 2),min_df=0.005,max_df=0.3)\n",
    "            #ocab = vectorizer.get_feature_names()\n",
    "            #print(\"Execution\")\n",
    "            X = vect.fit_transform(corpus)\n",
    "            if(is_tfidf == True):\n",
    "                X = TfidfTransformer().fit_transform(X)\n",
    "                prefix = prefix + \"norm_\"\n",
    "                \n",
    "            count = X.toarray().sum(axis=0)\n",
    "            idx = np.argsort(-count)\n",
    "            count = count[idx]\n",
    "\n",
    "            feature_name = np.array(vect.get_feature_names())[idx]\n",
    "            #plt.bar(range(len(count)), count)\n",
    "            #plt.show()\n",
    "\n",
    "            self.keyword_list = list(zip(feature_name[:num], count[:num]))\n",
    "            result = pd.DataFrame(self.keyword_list)\n",
    "            result.to_csv(\"./keyword_frequency_result/\" + prefix + \"keyword_frequency.csv\",encoding=\"UTF8\")            \n",
    "            \n",
    "        # 키워드 트랜드 분석\n",
    "        # norm = 년도별 정규화\n",
    "        # voca = \"분석할 단어 설정\"\n",
    "        def keyword_trend_analysis(self, prefix=\"\", norm=False, ma ='twitter2',voca = \"\"):\n",
    "            \n",
    "            if ma == \"twitter\":\n",
    "                self.morph_analyzer = self.twitter\n",
    "            if ma == \"twitter2\":\n",
    "                self.morph_analyzer = self.twitter2\n",
    "            \n",
    "            temp_df = self.filtered_idx_result_df\n",
    "            file_list = self.filtered_idx_result_df['file_list'].tolist()\n",
    "            \n",
    "            x = list()\n",
    "            \n",
    "            # 각 vocabulary에 대해서 수행\n",
    "            result = pd.DataFrame()\n",
    "            for v in voca:\n",
    "                #print(v)\n",
    "                fidx = []\n",
    "                didx = []\n",
    "                for i in range(len(file_list)):\n",
    "                    f = open(file_list[i], 'r',encoding='utf-8',errors=\"ignore\")\n",
    "                    doc = f.read()\n",
    "                    f.close()\n",
    "                    p = re.compile(v)\n",
    "                    ridx = p.search(doc)\n",
    "                    if(ridx == None):\n",
    "                        didx.append(i)\n",
    "                    else:\n",
    "                        fidx.append(i)\n",
    "                    \n",
    "                # 날짜 추출\n",
    "                dt_list = [self.filtered_idx_result_df['date'].tolist()[sidx] for sidx in fidx]\n",
    "                dt_cnt = collections.Counter(list(map(lambda x : x.split(\"-\")[0], dt_list)))\n",
    "            \n",
    "                x = list(dt_cnt.keys())\n",
    "                y = list(dt_cnt.values())\n",
    "                y = pd.DataFrame(y)\n",
    "                result = pd.concat([result,y],axis=1)\n",
    "                \n",
    "           # dt_list = self.filtered_idx_result_df['date'].tolist()   \n",
    "           # dt_cnt = collections.Counter(list(map(lambda x : x.split(\"-\")[0], dt_list)))        \n",
    "            \n",
    "            result.columns = voca\n",
    "\n",
    "            if(norm == True):\n",
    "                prefix = \"norm_\" + prefix\n",
    "                temp_df_list = self.filtered_idx_result_df['date'].tolist()\n",
    "                all_dt_cnt = collections.Counter(list(map(lambda x : x.split(\"-\")[0], temp_df_list)))\n",
    "                \n",
    "                all_y = list(all_dt_cnt.values())\n",
    "                for i in range(len(all_y)):\n",
    "                    #print(result)\n",
    "                    result.iloc[i,:] = result.iloc[i,:] / all_y[i]\n",
    "            \n",
    "            x = pd.DataFrame(x)\n",
    "            result = pd.concat([x,result],axis=1)\n",
    "                        \n",
    "            result.to_csv(\"./keyword_trend_result/\" + prefix + \"keyword_trends.csv\",encoding=\"UTF8\")\n",
    "    \n",
    "        # 토픽 모델링\n",
    "        def topic_modeling(self,n_topics, ma = \"twitter2\", prefix = \"\",random_seed=1,s_words=[]):\n",
    "            \n",
    "            if ma == \"twitter\":\n",
    "                self.morph_analyzer = self.twitter\n",
    "            if ma == \"twitter2\":\n",
    "                self.morph_analyzer = self.twitter2\n",
    "                       \n",
    "            # 형태소 분석을 기본적으로 수행\n",
    "            file_list = self.filtered_idx_result_df['file_list'].tolist()\n",
    "            #print(file_list)\n",
    "            corpus = [\"\"] * len(file_list)\n",
    "\n",
    "            # 파일 내용 읽어서 코퍼스에 저장\n",
    "\n",
    "            for i in range(len(file_list)):\n",
    "                #print(i)\n",
    "                f = open(file_list[i], 'r',encoding='utf-8',errors=\"ignore\")\n",
    "                doc = f.read()\n",
    "                f.close()\n",
    "                corpus[i] = \" \".join(self.morph_analyzer.nouns(doc))\n",
    "\n",
    "            #print(corpus)\n",
    "\n",
    "            # Stop words \n",
    "            stop_words = [\"전략\", \"연구\", \"평가\", \"마련\", \"조사\", \"관리\", \"보다\", \"분석\", \"구축\",\"개발\",\"정책\",\"대책\",\n",
    "                          \"관련\",\"전문가\",\"예정\",\"이번\",\"환경부\", \"환경\", \"기술\", \"사업\", \"산업\", \"기업\",\"우리나라\", \"또한\",\n",
    "                          \"총회\", \"대한\", \"통해\", \"한국\",\"국내\",\"개최\",\"행사\",\"우수\",\"서울\",\"장관\",\"호남권\",\"피해\",\"해외\",\"일자리\",\"박람회\",\n",
    "                          \"지원\", \"위해\", \"총회\",\"국제\",\"분야\",\"환경기술\"]\n",
    "  \n",
    "            stop_words = list(set(stop_words))\n",
    "    \n",
    "            if(len(s_words) != 0):\n",
    "                stop_words = s_words\n",
    "            vect = CountVectorizer(stop_words = stop_words,ngram_range=(1, 2),min_df=0.05,max_df=0.3)\n",
    "            X = vect.fit_transform(corpus)\n",
    "            X = TfidfTransformer().fit_transform(X)\n",
    "\n",
    "            #vect = CountVectorizer(stop_words = stop_words,min_df=0.005,max_df=0.3)\n",
    "\n",
    "            n_words = 300\n",
    "            for i in n_topics:\n",
    "                print(i)\n",
    "                # LDA 수횅\n",
    "                lda = LatentDirichletAllocation(n_components = i,learning_method=\"batch\",random_state=random_seed)\n",
    "                #lda = LatentDirichletAllocation(learning_method=\"batch\")\n",
    "                self.document_topics = lda.fit_transform(X)\n",
    "                \n",
    "                sorting = np.argsort(lda.components_, axis=1)[:,::-1]\n",
    "                feature_names = np.array(vect.get_feature_names())\n",
    "                \n",
    "                #mglearn.tools.print_topics(topics=range(i), feature_names = feature_names, sorting=sorting, topics_per_chunk=20, n_words=20)\n",
    "                #self.filtered_idx_result_df[\"topic\"] = np.argmax(self.document_topics,axis=1).tolist()\n",
    "                \n",
    "                self.topic_list = np.argmax(self.document_topics,axis=1).tolist()\n",
    "                \n",
    "                self.topic_words = pd.DataFrame(feature_names[sorting])\n",
    "                self.topic_words = self.topic_words.T\n",
    "                self.topic_words = self.topic_words[:n_words]\n",
    "                \n",
    "                # 파일이름 지정\n",
    "                filename =  \"./lda_result/\" +  str(prefix) + \"_result_n_\" + str(i) + \"_\" \n",
    "\n",
    "                # 파일 저장\n",
    "                self.document_topics = pd.DataFrame(self.document_topics)\n",
    "                self.document_topics.to_csv(filename + \"document_topic.csv\",header=None,index=False,encoding=\"UTF8\")\n",
    "                self.topic_words.to_csv(filename + \"topic_words.csv\",header=None,index=False,encoding=\"UTF8\")\n",
    "                \n",
    "                # 시각화 및 저장\n",
    "                self.topic_modeling_vis(lda, X, vect, filename)\n",
    "\n",
    "        def topic_modeling_vis(self,lda,dtm,vect,filename):\n",
    "            pyldavis = pyLDAvis.sklearn.prepare(lda, dtm,vect)\n",
    "            pyLDAvis.save_html(pyldavis,filename + \"lda.html\")\n",
    "            \n",
    "        def select_topics(self,topics):\n",
    "            #print(\"Select Topics\")\n",
    "            topic_list = self.topic_list\n",
    "            #print(topic_list)\n",
    "            tidx = list(map(lambda x: x in topics , topic_list))\n",
    "            #print(tidx)\n",
    "            \n",
    "            self.filtered_idx_result_df = self.filtered_idx_result_df.iloc[tidx,:]\n",
    "            self.filtered_idx_result_df = self.filtered_idx_result_df.reset_index(drop=True)\n",
    "            self.topic_list = np.array(self.topic_list)[tidx].tolist()\n",
    "            \n",
    "        def topic_modeling_statistics(self,prefix=\"\"):\n",
    "            \n",
    "            topic_list = self.topic_list\n",
    "            result = pd.DataFrame()\n",
    "\n",
    "            dt = None\n",
    "            for t in range(max(topic_list) + 1):\n",
    "                print(t)\n",
    "                sub_idx = (np.array(topic_list) == t)\n",
    "                sub_idx = list(sub_idx)\n",
    "                sub_result = self.filtered_idx_result_df.loc[sub_idx,:]\n",
    "                date_list = sub_result['date']\n",
    "                all_dt_cnt = collections.Counter(list(map(lambda x : x.split(\"-\")[0], date_list)))\n",
    "                all_y = list(all_dt_cnt.values())\n",
    "                all_y = pd.DataFrame(all_y)\n",
    "                result = pd.concat([result,all_y],axis=1)            \n",
    "\n",
    "            dt = list(all_dt_cnt.keys())\n",
    "            dt = pd.DataFrame(dt)\n",
    "            result = pd.concat([dt,result],axis=1)\n",
    "\n",
    "            result.to_csv(\"./lda_stat/\" + prefix + \"lda_stat.csv\",encoding=\"UTF8\")\n",
    "        \n",
    "            \n",
    "        def extract_sentence(self,w1,ma=\"twitter2\",prefix=\"\"):\n",
    "            if ma == \"twitter\":\n",
    "                self.morph_analyzer = self.twitter\n",
    "            if ma == \"twitter2\":\n",
    "                self.morph_analyzer = self.twitter2  \n",
    "\n",
    "            file_list =  self.filtered_idx_result_df['file_list'].tolist()\n",
    "            filtered_sentences= []\n",
    "            \n",
    "            for i in range(len(file_list)):\n",
    "                f = open(file_list[i], 'r',encoding='utf-8',errors=\"ignore\")\n",
    "                doc = f.read()\n",
    "                sentences = self.split_text(doc)\n",
    "                \n",
    "                for s in sentences:\n",
    "                    filtered_sentences.append(s)\n",
    "            \n",
    "            self.temp = pd.DataFrame(filtered_sentences)\n",
    "            \n",
    "            for w in w1:\n",
    "                temp = []\n",
    "                for s in filtered_sentences :\n",
    "                    p = re.compile(w)\n",
    "                    ridx = p.search(s)\n",
    "\n",
    "                    if(ridx == None):\n",
    "                        continue   \n",
    "                    s = s.strip()\n",
    "                    temp.append(s)\n",
    "                filtered_sentences = temp\n",
    "            \n",
    "            self.filtered_sentences = filtered_sentences\n",
    "            result = pd.DataFrame(self.filtered_sentences)\n",
    "            \n",
    "            result.to_csv(\"./filtered_sentences_result/\" + prefix + \"filtered_sentences.csv\")\n",
    "            \n",
    "        # word2vec 학습\n",
    "        def training_keyword_w2v(self,ma=\"twitter2\"):\n",
    "            file_list = self.filtered_idx_result_df['file_list'].tolist()\n",
    "            corpus = [\"\"] * len(file_list)\n",
    "            # 형태소 분석기 선택\n",
    "\n",
    "            if ma == \"twitter\":\n",
    "                self.morph_analyzer = self.twitter\n",
    "            if ma == \"twitter2\":\n",
    "                self.morph_analyzer = self.twitter\n",
    "                \n",
    "            # 파일 내용 읽어서 코퍼스에 저장\n",
    "            for i in range(len(file_list)):\n",
    "                #print(i)\n",
    "                f = open(file_list[i], 'r',encoding='utf-8',errors=\"ignore\")\n",
    "                doc = f.read()\n",
    "                f.close()\n",
    "                doc = re.sub('[^가-힝0-9a-zA-Z\\\\s]', '', doc)\n",
    "                corpus[i] = self.morph_analyzer.nouns(doc)\n",
    "\n",
    "            model = Word2Vec(corpus)\n",
    "            model.init_sims(replace=True)\n",
    "            w2v_model = model\n",
    "            \n",
    "        # word2vec 학습 결과 저장\n",
    "        def w2v_analysis(self,word=[],word2=[],num=100):\n",
    "            #print(self.w2v_model.most_similatr(word,topn=num))\n",
    "            result = pd.DataFrame(self.w2v_model.most_similar(positive=word,negative=word2,topn=num))\n",
    "            #print(self.w2v_model.predict_output_word(word, topn=num))\n",
    "            filename = \"\".join(word)\n",
    "            result.to_csv(\"./w2v_result/\" + \"w2v_\" + filename + \".csv\",encoding=\"UTF8\")\n",
    "            return result\n",
    "\n",
    "        \n",
    "        # 문서 레벨 동시 출현 단어\n",
    "        def doc_level_coccurrence_words(self,w1, num=100,ma=\"twitter2\",prefix=\"\",s_words=[]):\n",
    "            \n",
    "            if ma == \"twitter\":\n",
    "                self.morph_analyzer = self.twitter\n",
    "            if ma == \"twitter2\":\n",
    "                self.morph_analyzer = self.twitter2  \n",
    "                \n",
    "            # 문서추출\n",
    "            file_list =  self.filtered_idx_result_df['file_list'].tolist()\n",
    "            temp_idx_result_df = pd.DataFrame()\n",
    "            #print(file_list)\n",
    "            \n",
    "            fidx = []\n",
    "            didx = []\n",
    "            for w in w1 :\n",
    "                for i in range(len(file_list)):\n",
    "                    f = open(file_list[i], 'r',encoding='utf-8',errors=\"ignore\")\n",
    "                    doc = f.read()\n",
    "                    p = re.compile(w)\n",
    "                    ridx = p.search(doc)\n",
    "\n",
    "                    if(ridx == None):\n",
    "                       #print(\"없음\")\n",
    "                        didx.append(i)\n",
    "\n",
    "                    else:\n",
    "                        fidx.append(i)\n",
    "                        \n",
    "            fidx = list(set(fidx))\n",
    "            print(fidx)\n",
    "            temp_idx_result_df = self.filtered_idx_result_df.iloc[fidx]\n",
    "            temp_idx_result_df = temp_idx_result_df.reset_index(drop=True)\n",
    "            \n",
    "            file_list =  temp_idx_result_df['file_list'].tolist()\n",
    "            \n",
    "            corpus = []     \n",
    "            for i in range(len(file_list)):\n",
    "                f = open(file_list[i], 'r',encoding='utf-8',errors=\"ignore\")\n",
    "                doc = f.read()\n",
    "                \n",
    "                # 파일 내용 읽어서 코퍼스에 저장\n",
    "                corpus.append(\" \".join(self.morph_analyzer.nouns(doc)))\n",
    "\n",
    "            stop_words = [\"전략\", \"연구\", \"평가\", \"마련\", \"조사\", \"관리\", \"보다\", \"분석\", \"구축\",\"개발\",\"정책\",\"대책\",\n",
    "                          \"관련\",\"전문가\",\"예정\",\"이번\",\"환경부\", \"환경\", \"기술\", \"사업\", \"산업\", \"기업\",\"우리나라\", \"또한\",\n",
    "                          \"총회\", \"대한\", \"통해\", \"한국\",\"국내\",\"개최\",\"행사\",\"우수\",\"서울\",\"장관\",\"호남권\",\"피해\",\"해외\",\"일자리\",\"박람회\",\n",
    "                          \"지원\", \"위해\", \"총회\",\"국제\",\"분야\",\"환경기술\",\"10\",\"20\", \"지정\",\"전국\",\"선정\",\"사용\",\"다양\",\"하게\", \"대상\",\"대상으로\",\"제품\", \"공단\",\"이상\",\"정보\",\"대해\",\"국민\",\"지난\",\"올해\",\"현재\",\"1부\",\n",
    "                          \"관한\",\"이용\",\"증가\",\"협력\",\"도입\",\"향후\",\"발표\",\"이라\",\"참여\",\"포함\",\"수도권\",\"된다\", \"있으며\",\"1부\",\"등이\",\"이후\",\"15\",\"12\",\"정부\", \"회의\",\"기여\",\"활성화\",\"수준\" \"진행\",\"처리\",\"내용\",\"나타났다\",\n",
    "                          \"대비\",\"하였다\",\"2010년\",\"2011년\",\"2012년\",\"2013년\",\"2014년\",\"2015년\",\"2016년\",\"2017년\",\"실천\",\"말했다\",\"현장\",\n",
    "                          \"구성\",\"최초\",\"확산\",\"홈페이지\",\"가능\",\"사진\",\"나타\",\"설명\",\"6월\",\"제공\",\"효과\",\"우리\",\"kr\",\"10월\",\"계기\",\n",
    "                          \"거나\",\"평균\",\"이하\",\"예방\",\"기관\",\"수행\",\"5월\",\"사례\",\"업체\",\"사장\",\"대해서\",\"최근\",\"원장\",\"추가\",\"9월\",\"노력\",\n",
    "                          \"기존\",\"했다\",\"직접\",\"새로\",\"참여하\",\"소개\",\"7월\",\"4월\",\"시작\",\"윤성규\",\"이에\",\"4월\",\"통한\",\"성과\",\"관심\",\"가장\",\n",
    "                          \"적용\",\"하고\",\"과정\",\"보급\",\"동안\",\"연간\",\"거쳐\",\"사업장\",\"주요\",\"시행\",\"프로그램\",\"시설\",\"국민\",\"국립환경과학원\",\n",
    "                          \"국립생물자원관\",\"국립공원관리공단\",\"한펀\",\"논의\",\"공모전\",\"자료\",\"발견\",\"서식\",\"현황\",\"국가\",\"기대\",\"대하\",\n",
    "                          \"활동\",\"확보\",\"세계\",\"국립생태원\",\"홍보\",\"수준\",\"진행\",\"체결\",\"개정\",\"수립\",\"주민\",\"필요\",\"한편\",\"13\",\"5개\",\"11\",\"지역\",\n",
    "                          \"11월\",\"고려\",\"질의응답\",\"서비스\",\"같은\",\"적극\",\"하도\",\"이나\",\"한다\",\"17\",\"16\",\"운영\",\"확대\",\"강화\",\"활용\",\"확인\",\"계획\",\n",
    "                          \"설치\",\"발생\",\"개선\",\"기준\",\"제도\",\"공동\"]\n",
    "\n",
    "            if(len(s_words) != 0):\n",
    "                stop_words = s_words  \n",
    "            vect = CountVectorizer(stop_words = stop_words,ngram_range=(1, 2),min_df=0.005,max_df=0.3)\n",
    "       \n",
    "            X = vect.fit_transform(corpus)\n",
    "\n",
    "            count = X.toarray().sum(axis=0)\n",
    "            idx = np.argsort(-count)\n",
    "            count = count[idx]\n",
    "\n",
    "            feature_name = np.array(vect.get_feature_names())[idx]\n",
    "            #plt.bar(range(len(count)), count)\n",
    "            #plt.show()\n",
    "\n",
    "            co_word_list = list(zip(feature_name[:num], count[:num]))\n",
    "            co_word_list = pd.DataFrame(co_word_list)\n",
    "            co_word_list.to_csv(\"./doc_level_cowords/\" + prefix + \"coccurrence_words\" + \".csv\",encoding=\"UTF8\")\n",
    "        \n",
    "        # 문장레벨 동시 출현 단어\n",
    "        def sentence_level_coccurrence_words(self,w1, num=100, ma=\"twitter2\",prefix=\"\",s_words=[]):\n",
    "            \n",
    "            if ma == \"twitter\":\n",
    "                self.morph_analyzer = self.twitter\n",
    "            if ma == \"twitter2\":\n",
    "                self.morph_analyzer = self.twitter2  \n",
    "\n",
    "            file_list =  self.filtered_idx_result_df['file_list'].tolist()\n",
    "            filtered_sentences= []\n",
    "            for i in range(len(file_list)):\n",
    "                f = open(file_list[i], 'r',encoding='utf-8',errors=\"ignore\")\n",
    "                doc = f.read()\n",
    "                sentences = self.split_text(doc)\n",
    "                \n",
    "                for s in sentences:\n",
    "                    filtered_sentences.append(s)\n",
    "            \n",
    "            self.temp = pd.DataFrame(filtered_sentences)\n",
    "            \n",
    "            for w in w1:\n",
    "                temp = []\n",
    "                for s in filtered_sentences :\n",
    "                    p = re.compile(w)\n",
    "                    ridx = p.search(s)\n",
    "\n",
    "                    if(ridx == None):\n",
    "                        continue   \n",
    "                    s = s.strip()\n",
    "                    temp.append(s)\n",
    "                filtered_sentences = temp\n",
    "            \n",
    "            self.temp = pd.DataFrame(filtered_sentences)\n",
    "            print(self.temp)\n",
    "            # 파일 내용 읽어서 코퍼스에 저장\n",
    "    \n",
    "            corpus = []     \n",
    "            for i in range(len(filtered_sentences)):\n",
    "                corpus.append(\" \".join(self.morph_analyzer.nouns(filtered_sentences[i])))\n",
    "                \n",
    "                \n",
    "            stop_words = [\"전략\", \"연구\", \"평가\", \"마련\", \"조사\", \"관리\", \"보다\", \"분석\", \"구축\",\"개발\",\"정책\",\"대책\",\n",
    "              \"관련\",\"전문가\",\"예정\",\"이번\",\"환경부\", \"환경\", \"기술\", \"사업\", \"산업\", \"기업\",\"우리나라\", \"또한\",\n",
    "              \"총회\", \"대한\", \"통해\", \"한국\",\"국내\",\"개최\",\"행사\",\"우수\",\"서울\",\"장관\",\"호남권\",\"피해\",\"해외\",\"일자리\",\"박람회\",\n",
    "              \"지원\", \"위해\", \"총회\",\"국제\",\"분야\",\"환경기술\",\"10\",\"20\", \"지정\",\"전국\",\"선정\",\"사용\",\"다양\",\"하게\", \"대상\",\"대상으로\",\"제품\", \"공단\",\"이상\",\"정보\",\"대해\",\"국민\",\"지난\",\"올해\",\"현재\",\"1부\",\n",
    "              \"관한\",\"이용\",\"증가\",\"협력\",\"도입\",\"향후\",\"발표\",\"이라\",\"참여\",\"포함\",\"수도권\",\"된다\", \"있으며\",\"1부\",\"등이\",\"이후\",\"15\",\"12\",\"정부\", \"회의\",\"기여\",\"활성화\",\"수준\" \"진행\",\"처리\",\"내용\",\"나타났다\",\n",
    "              \"대비\",\"하였다\",\"2010년\",\"2011년\",\"2012년\",\"2013년\",\"2014년\",\"2015년\",\"2016년\",\"2017년\",\"실천\",\"말했다\",\"현장\",\n",
    "              \"구성\",\"최초\",\"확산\",\"홈페이지\",\"가능\",\"사진\",\"나타\",\"설명\",\"6월\",\"제공\",\"효과\",\"우리\",\"kr\",\"10월\",\"계기\",\n",
    "              \"거나\",\"평균\",\"이하\",\"예방\",\"기관\",\"수행\",\"5월\",\"사례\",\"업체\",\"사장\",\"대해서\",\"최근\",\"원장\",\"추가\",\"9월\",\"노력\",\n",
    "              \"기존\",\"했다\",\"직접\",\"새로\",\"참여하\",\"소개\",\"7월\",\"4월\",\"시작\",\"윤성규\",\"이에\",\"4월\",\"통한\",\"성과\",\"관심\",\"가장\",\n",
    "              \"적용\",\"하고\",\"과정\",\"보급\",\"동안\",\"연간\",\"거쳐\",\"사업장\",\"주요\",\"시행\",\"프로그램\",\"시설\",\"국민\",\"국립환경과학원\",\n",
    "              \"국립생물자원관\",\"국립공원관리공단\",\"한펀\",\"논의\",\"공모전\",\"자료\",\"발견\",\"서식\",\"현황\",\"국가\",\"기대\",\"대하\",\n",
    "              \"활동\",\"확보\",\"세계\",\"국립생태원\",\"홍보\",\"수준\",\"진행\",\"체결\",\"개정\",\"수립\",\"주민\",\"필요\",\"한편\",\"13\",\"5개\",\"11\",\"지역\",\n",
    "              \"11월\",\"고려\",\"질의응답\",\"서비스\",\"같은\",\"적극\",\"하도\",\"이나\",\"한다\",\"17\",\"16\",\"운영\",\"확대\",\"강화\",\"활용\",\"확인\",\"계획\",\n",
    "              \"설치\",\"발생\",\"개선\",\"기준\",\"제도\",\"공동\"]\n",
    "\n",
    "\n",
    "            if(len(s_words) != 0):\n",
    "                stop_words = s_words  \n",
    "                \n",
    "            vect = CountVectorizer(stop_words = stop_words,ngram_range=(1, 2),min_df=0.005,max_df=0.3)\n",
    "       \n",
    "            X = vect.fit_transform(corpus)\n",
    "\n",
    "            count = X.toarray().sum(axis=0)\n",
    "            idx = np.argsort(-count)\n",
    "            count = count[idx]\n",
    "\n",
    "            feature_name = np.array(vect.get_feature_names())[idx]\n",
    " \n",
    "            co_word_list = list(zip(feature_name[:num], count[:num]))\n",
    "            co_word_list = pd.DataFrame(co_word_list)\n",
    "            co_word_list.to_csv(\"./sentence_level_cowords/\" + prefix + \"cowords\" + \".csv\",encoding=\"UTF8\")\n",
    "            \n",
    "        # 명사 인식\n",
    "        def add_unknown_nouns(self):\n",
    "            file_list = self.filtered_idx_result_df['file_list'].tolist()\n",
    "            corpus = [] \n",
    "            for i in range(len(file_list)):\n",
    "                f = open(file_list[i], 'r',encoding='utf-8',errors=\"ignore\")\n",
    "                doc = f.read()\n",
    "                doc = doc.replace(\"\\n\",\"\")\n",
    "                corpus.append(\"\".join(doc))\n",
    "\n",
    "            corpus = pd.DataFrame(corpus)\n",
    "            corpus.to_csv(\"./corpus.txt\",sep=\" \", header=None)\n",
    "            \n",
    "            corpus_fname = './corpus.txt'\n",
    "            sents = DoublespaceLineCorpus(corpus_fname, iter_sent=True)\n",
    "\n",
    "            noun_extractor = LRNounExtractor_v2(verbose=False)\n",
    "            noun_extractor.train(sents)\n",
    "            nouns = noun_extractor.extract()\n",
    "            nouns = list(nouns.keys())\n",
    "            self.nouns = nouns\n",
    "            self.twitter.add_dictionary(nouns, 'Noun')\n",
    "                \n",
    "           # nouns\n",
    "        \n",
    "        def sentence_summarization(self,cnt=10):\n",
    "            \n",
    "            t_list = []\n",
    "            for s in self.filtered_sentences:\n",
    "                t= re.sub('[^가-힝0-9a-zA-Z\\\\s]', '', s)\n",
    "                t_list.append(t)\n",
    "                \n",
    "            p_sentences = \". \".join(t_list)\n",
    "            textrank = TextRank(b)\n",
    "            t = textrank.summarize(count=cnt)\n",
    "            t = a.split(\"\\n\")\n",
    "            \n",
    "            t = pd.DataFrame(t)\n",
    "            t.columns=['Sentences']\n",
    "            result.to_csv(\"./sentence_summary_result/\" + \"ss_\" + filename + \".csv\",encoding=\"UTF8\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\"전략\", \"연구\", \"평가\", \"마련\", \"조사\", \"관리\", \"보다\", \"분석\", \"구축\",\"개발\",\"정책\",\"대책\",\n",
    "              \"관련\",\"전문가\",\"예정\",\"이번\",\"환경부\", \"환경\", \"기술\", \"사업\", \"산업\", \"기업\",\"우리나라\", \"또한\",\n",
    "              \"총회\", \"대한\", \"통해\", \"한국\",\"국내\",\"개최\",\"행사\",\"우수\",\"장관\",\"피해\",\"해외\",\"일자리\",\n",
    "              \"박람회\",\"지원\", \"위해\", \"총회\",\"국제\",\"분야\",\"환경기술\",\"10\",\"20\", \"지정\",\"전국\",\"선정\",\"사용\",\"다양\",\"하게\",\n",
    "              \"대상\",\"대상으로\",\"제품\", \"공단\",\"이상\",\"정보\",\"대해\",\"국민\",\"지난\",\"올해\",\"현재\",\"1부\",\"관한\",\"이용\",\"증가\",\n",
    "              \"협력\",\"도입\",\"향후\",\"발표\",\"이라\",\"참여\",\"포함\",\"된다\", \"있으며\",\"1부\",\"등이\",\"이후\",\"15\",\"12\",\n",
    "              \"정부\", \"회의\",\"기여\",\"활성화\",\"수준\" \"진행\",\"처리\",\"내용\",\"나타났다\", \"대비\",\"하였다\",\"2010년\",\"2011년\",\n",
    "              \"2012년\",\"2013년\",\"2014년\",\"2015년\",\"2016년\",\"2017년\",\"실천\",\"말했다\",\"현장\",\"구성\",\"최초\",\"확산\",\"홈페이지\",\n",
    "              \"가능\",\"사진\",\"나타\",\"설명\",\"6월\",\"제공\",\"효과\",\"우리\",\"kr\",\"10월\",\"계기\", \"거나\",\"평균\",\"이하\",\"예방\",\"기관\",\n",
    "              \"수행\",\"5월\",\"사례\",\"업체\",\"사장\",\"대해서\",\"최근\",\"원장\",\"추가\",\"9월\",\"노력\",\"기존\",\"했다\",\"직접\",\"새로\",\n",
    "              \"참여하\",\"소개\",\"7월\",\"4월\",\"시작\",\"윤성규\",\"이에\",\"4월\",\"통한\",\"성과\",\"관심\",\"가장\", \"적용\",\"하고\",\"과정\",\n",
    "              \"보급\",\"동안\",\"연간\",\"거쳐\",\"사업장\",\"주요\",\"시행\",\"프로그램\",\"시설\",\"국민\",\"국립환경과학원\",\"국립생물자원관\",\n",
    "              \"국립공원관리공단\",\"한펀\",\"논의\",\"공모전\",\"자료\",\"발견\",\"서식\",\"현황\",\"국가\",\"기대\",\"대하\", \"활동\",\"확보\",\"세계\",\n",
    "              \"국립생태원\",\"홍보\",\"수준\",\"진행\",\"체결\",\"개정\",\"수립\",\"주민\",\"필요\",\"한편\",\"13\",\"5개\",\"11\",\"지역\",\n",
    "              \"11월\",\"고려\",\"질의응답\",\"서비스\",\"같은\",\"적극\",\"하도\",\"이나\",\"한다\",\"17\",\"16\",\"운영\",\"확대\",\"강화\",\"활용\",\"확인\",\n",
    "              \"계획\",\"설치\",\"발생\",\"개선\",\"기준\",\"제도\",\"공동\"]\n",
    "stop_words = list(set(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pyLDAvis/_prepare.py:387: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  topic_term_dists = topic_term_dists.ix[topic_order]\n",
      "/usr/local/lib/python3.6/dist-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    }
   ],
   "source": [
    "# 키워드 기반 기후변화 문서 정제\n",
    "\n",
    "ma1 = MeReportAnalyzer()\n",
    "ma1.filtering_date('2005-01-01','2017-12-31')\n",
    "ma1.filtering_contents(\"인사발령|공개모집|필기시험|발령|인사 발령|과장|전보 일자|공무원|대변인|국장|승진|공모전|캠프 캐롤|고엽제 매립\",True)\n",
    "ma1.filtering_contents(ma1.climate_words,False)\n",
    "\n",
    "ma2 = MeReportAnalyzer()\n",
    "ma2.filtering_date('2005-01-01','2017-12-31')\n",
    "ma2.filtering_contents(\"인사발령|공개모집|필기시험|발령|인사 발령|과장|전보 일자|공무원|대변인|국장|승진|공모전|캠프 캐롤|고엽제 매립\",True)\n",
    "ma2.topic_modeling([5],prefix = \"f2\",random_seed=2)\n",
    "ma2.select_topics([3])\n",
    "\n",
    "ma = ma1\n",
    "ma.combine_idx_result(ma2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
