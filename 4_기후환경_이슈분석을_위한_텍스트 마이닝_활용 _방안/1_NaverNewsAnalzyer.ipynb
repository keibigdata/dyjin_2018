{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import gensim \n",
    "import mglearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import konlpy\n",
    "import ckonlpy\n",
    "from konlpy.utils import pprint\n",
    "from konlpy.tag import Hannanum\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from konlpy.tag import Kkma, Mecab\n",
    "import pyLDAvis.gensim\n",
    "import pyLDAvis.sklearn\n",
    "import collections\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import copy\n",
    "import soynlp\n",
    "import time\n",
    "from soynlp.utils import DoublespaceLineCorpus\n",
    "from soynlp.noun import LRNounExtractor_v2\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font', family='NanumBarunGothicOTF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaverNewsAnalyzer():\n",
    "\n",
    "        def __init__ (self):\n",
    "            self.idx_result_df = pd.DataFrame()\n",
    "\n",
    "            input_file_name = \"./result/indexing.txt\"\n",
    "\n",
    "            self.idx_result_df = pd.read_csv(input_file_name,encoding=\"UTF8\",header=None)\n",
    "            self.filtered_idx_result_df = pd.DataFrame()\n",
    "            self.idx_result_df.columns = ['date','title','press','content_url_list','file_list']\n",
    "\n",
    "            self.climate_words = ['기후변화','가뭄', '강추위', '결빙', '그린란드빙하', '기후', '남극빙하', '녹색성장', '대설', '라니냐',\n",
    "                                  '무더위', '북극빙하', '사막화', '산성비', '쓰나미', '엘니뇨', '열대야', '열섬', '열파', '온난',\n",
    "                                  '온실가스', '우박', '이산화탄소', '이상고온', '이상기온', '이상저온', '장마', '적설', '집중강우',\n",
    "                                  '집중호우', '침수', '탄소', '태풍', '파랑', '패랑', '폭설', '폭염', '폭우', '풍량', '풍수해', '한파',\n",
    "                                  '해수면', '해일', '혹서', '혹한', '홍수', '황사비', '히말라야빙하']\n",
    "\n",
    "            self.climate_words = \"|\".join(self.climate_words)\n",
    "            self.kkma = Kkma()\n",
    "            self.twitter = ckonlpy.tag.Twitter()\n",
    "            self.twitter2 = konlpy.tag.Twitter()\n",
    "            \n",
    "            self.document_topics = None\n",
    "            self.topic_words = None\n",
    "            self.filtered_sentences = None\n",
    "            self.morph_analyzer = None\n",
    "            self.w2v_model = None\n",
    "            self.keyword_list = None\n",
    "            self.topic_list = None\n",
    "            self.corpus = None\n",
    "            self.nouns  = None\n",
    "            self.temp = None\n",
    "            \n",
    "            # 결과저장 디렉토리 생성\n",
    "            dirname = 'result'\n",
    "            if ((os.path.isdir('./' + dirname + '/')) == False):\n",
    "                os.mkdir('./' + dirname + '/')\n",
    "                \n",
    "            dirname = './analysis_result'\n",
    "            if ((os.path.isdir('./' + dirname + '/')) == False):\n",
    "                os.mkdir('./' + dirname + '/')\n",
    "                \n",
    "            dirname = './lda_result'\n",
    "            if ((os.path.isdir('./' + dirname + '/')) == False):\n",
    "                os.mkdir('./' + dirname + '/')\n",
    "\n",
    "            dirname = './doc_trend_result'\n",
    "            if ((os.path.isdir('./' + dirname + '/')) == False):\n",
    "                os.mkdir('./' + dirname + '/')\n",
    "                \n",
    "            dirname = './keyword_frequency_result'\n",
    "            if ((os.path.isdir('./' + dirname + '/')) == False):\n",
    "                os.mkdir('./' + dirname + '/')\n",
    "            \n",
    "            dirname = './keyword_trend_result'\n",
    "            if ((os.path.isdir('./' + dirname + '/')) == False):\n",
    "                os.mkdir('./' + dirname + '/')\n",
    "                \n",
    "            dirname = './lda_stat'\n",
    "            if ((os.path.isdir('./' + dirname + '/')) == False):\n",
    "                os.mkdir('./' + dirname + '/')    \n",
    "                \n",
    "            dirname = './lda_result'\n",
    "            if ((os.path.isdir('./' + dirname + '/')) == False):\n",
    "                os.mkdir('./' + dirname + '/')\n",
    "                \n",
    "            dirname = './sentence_level_cowords'\n",
    "            if ((os.path.isdir('./' + dirname + '/')) == False):\n",
    "                os.mkdir('./' + dirname + '/')\n",
    "            \n",
    "            dirname = './doc_level_cowords'\n",
    "            if ((os.path.isdir('./' + dirname + '/')) == False):\n",
    "                os.mkdir('./' + dirname + '/')\n",
    "                \n",
    "            dirname = './w2v_result'\n",
    "            if ((os.path.isdir('./' + dirname + '/')) == False):\n",
    "                os.mkdir('./' + dirname + '/')\n",
    "                \n",
    "            dirname = './filtered_sentence_result'\n",
    "            if ((os.path.isdir('./' + dirname + '/')) == False):\n",
    "                os.mkdir('./' + dirname + '/')    \n",
    "                \n",
    "            dirname = './press_statistic_result/'\n",
    "            if ((os.path.isdir('./' + dirname + '/')) == False):\n",
    "                os.mkdir('./' + dirname + '/')  \n",
    "                \n",
    "            dirname = './filtered_idx_result/'\n",
    "            if ((os.path.isdir('./' + dirname + '/')) == False):\n",
    "                os.mkdir('./' + dirname + '/')     \n",
    "            \n",
    "                \n",
    "            # 네이버 추가 전처리    \n",
    "            self.idx_result_df['date'] = list(map(str, self.idx_result_df['date']))\n",
    "            self.idx_result_df['file_list'] = list(map(lambda x: os.path.dirname(input_file_name) + \"/\" + os.path.basename(x), self.idx_result_df['file_list'].tolist()))\n",
    "\n",
    "            self.filtered_idx_result_df =  self.idx_result_df\n",
    "            \n",
    "            # remove duplicate\n",
    "            self.remove_duplicate()\n",
    "            \n",
    "        def remove_duplicate(self):\n",
    "            self.filtered_idx_result_df = self.filtered_idx_result_df.drop_duplicates(subset=['title','date'],keep=\"last\")\n",
    "            self.filtered_idx_result_df = self.filtered_idx_result_df.reset_index(drop=True)\n",
    "        \n",
    "        def press_statistics(self, norm=False, prefix=\"\"):\n",
    "            \n",
    "            # 언론사 추출\n",
    "            press_list = set(self.filtered_idx_result_df['press'].tolist())\n",
    "            # 날짜 추출\n",
    "            \n",
    "            result_df = pd.DataFrame(columns=press_list,index=['2005','2006','2007','2008','2009','2010','2011','2012',\n",
    "                                                               '2013','2014','2015','2016','2017'])\n",
    "            \n",
    "            x = []\n",
    "            for press in press_list:\n",
    "                print(press)\n",
    "                press_idx = self.filtered_idx_result_df['press'] == press\n",
    "                temp_filtered_idx_result_df = self.filtered_idx_result_df.loc[press_idx]\n",
    "                \n",
    "                dt_list = temp_filtered_idx_result_df['date'].tolist()\n",
    "                dt_cnt = collections.Counter(list(map(lambda x : x.split(\"-\")[0], dt_list)))\n",
    "\n",
    "                x = list(dt_cnt.keys())\n",
    "                print(x)\n",
    "                y = list(dt_cnt.values())\n",
    "                print(y)\n",
    "\n",
    "                result_df[press][x] = y\n",
    "        \n",
    "            result_df = result_df.transpose()\n",
    "            result_df = result_df.fillna(0)\n",
    "            result_df.to_csv(\"./press_statistic_result/\" + prefix +  \"press_trends.csv\",encoding=\"UTF8\")\n",
    "                \n",
    "        # 날짜 필터링\n",
    "        def filtering_date(self,start_date = None, end_date = None):\n",
    "            dt_index = pd.date_range(start=start_date, end = end_date)\n",
    "            dt_list = dt_index.strftime(\"%Y-%m-%d\").tolist()\n",
    "            idx = list(map(lambda x: x in dt_list, self.filtered_idx_result_df['date'].tolist()))\n",
    "            self.filtered_idx_result_df = self.filtered_idx_result_df.iloc[idx]\n",
    "            self.filtered_idx_result_df = self.filtered_idx_result_df.reset_index(drop=True)\n",
    "            #print(self.idx_result_df[self.idx_result_df['date'] in dt_list]) \n",
    "                        \n",
    "        # 문서 필터링\n",
    "        # is_del = True : 제거 필터링 / is_del = False : 포함 필터링\n",
    "        def filtering_contents(self,rexp,is_del=False):\n",
    "            file_list =  self.filtered_idx_result_df['file_list'].tolist()\n",
    "            #print(file_list)\n",
    "            fidx = []\n",
    "            didx = []\n",
    "            for i in range(len(file_list)):\n",
    "                f = open(file_list[i], 'r',encoding='utf-8',errors=\"ignore\")\n",
    "                doc = f.read()\n",
    "                f.close()\n",
    "                p = re.compile(rexp)\n",
    "                ridx = p.search(doc)\n",
    "                    \n",
    "                if(ridx == None):\n",
    "                   #print(\"없음\")\n",
    "                    didx.append(i)\n",
    "                \n",
    "                else:\n",
    "                    fidx.append(i)\n",
    "                \n",
    "            #print(fidx)\n",
    "            if is_del == False:\n",
    "                #rint(fidx)\n",
    "                self.filtered_idx_result_df = self.filtered_idx_result_df.iloc[fidx]\n",
    "                self.filtered_idx_result_df = self.filtered_idx_result_df.reset_index(drop=True)\n",
    "            else :\n",
    "                self.filtered_idx_result_df = self.filtered_idx_result_df.iloc[didx]\n",
    "                self.filtered_idx_result_df = self.filtered_idx_result_df.reset_index(drop=True)\n",
    "            #print(self.idx_result_df)\n",
    "        \n",
    "\n",
    "        # 필터링 된 결과 저장 \n",
    "            \n",
    "        def load_lda_result(self,prefix=\"\"):\n",
    "            self.document_topics = pd.read_csv(\"./lda_result/\" + prefix + \"document_topic.csv\",encoding=\"UdTF8\",header=None)\n",
    "            self.topic_words = pd.read_csv(\"./lda_result/\" + prefix + \"topic_words.csv\",encoding=\"UTF8\",header=None)\n",
    "                            \n",
    "            self.topic_list = np.argmax(np.array(self.document_topics),axis=1).tolist()\n",
    "\n",
    "            \n",
    "        # 두개의 인덱스 파일 결합   \n",
    "        def combine_idx_result(self, prev):\n",
    "            #print(\"Current\")\n",
    "            #print(self.filtered_idx_result_df)\n",
    "            #print(\"Prev Index result\")\n",
    "            #print(prev.filtered_idx_result_df)\n",
    "            self.filtered_idx_result_df = pd.concat([self.filtered_idx_result_df,prev.filtered_idx_result_df])\n",
    "            self.filtered_idx_result_df = self.filtered_idx_result_df.drop_duplicates(['date','title'])\n",
    "            self.filtered_idx_result_df = self.filtered_idx_result_df.sort_values([\"file_list\"], ascending=True)\n",
    "            self.filtered_idx_result_df = self.filtered_idx_result_df.reset_index(drop=True)\n",
    "            \n",
    "        # 문서 트랜드 분석\n",
    "        def doc_trend_analysis(self, norm=False, prefix=\"\",title=\"네이버 환경뉴스\",xlab=\"년도\",ylab=\"문서수\"):\n",
    "            \n",
    "            # 날짜 추출\n",
    "            dt_list = self.filtered_idx_result_df['date'].tolist()\n",
    "            dt_cnt = collections.Counter(list(map(lambda x : x.split(\"-\")[0], dt_list)))\n",
    "         \n",
    "            x = list(dt_cnt.keys())\n",
    "            y = list(dt_cnt.values())\n",
    "            \n",
    "            # norm= True (정규화, 상대적 비교)\n",
    "            if(norm == True) :\n",
    "                prefix = prefix + \"norm_\"\n",
    "                dt_index = pd.date_range(start=min(dt_list), end = max(dt_list))\n",
    "                all_dt_list = dt_index.strftime(\"%Y-%m-%d\").tolist()\n",
    "                idx = list(map(lambda x: x in all_dt_list, self.idx_result_df['date'].tolist()))\n",
    "                temp_df = self.idx_result_df.iloc[idx]\n",
    "                temp_df = temp_df.reset_index(drop=True)\n",
    "                temp_df_list = temp_df['date'].tolist()\n",
    "                #print(temp_df)\n",
    "\n",
    "                all_dt_cnt = collections.Counter(list(map(lambda x : x.split(\"-\")[0], temp_df_list)))\n",
    "                all_y = list(all_dt_cnt.values())\n",
    "\n",
    "                y =  np.array(y) / np.array(all_y)\n",
    "                y =  y.tolist()\n",
    "\n",
    "            plt.xticks(rotation=50)\n",
    "            plt.plot(x,y,c=\"b\", lw=2, ls=\"--\", marker=\"o\", ms=10, mec=\"g\", mew=2, mfc=\"r\")\n",
    "            plt.title(title)\n",
    "            plt.xlabel(xlab)\n",
    "            plt.ylabel(ylab)\n",
    "            plt.savefig(\"./doc_trend_result/\" + prefix + \"doc_trends.png\",dpi=200)\n",
    "\n",
    "            result = pd.DataFrame([x,y]).transpose()\n",
    "            result.to_csv(\"./doc_trend_result/\" + prefix + \"doc_trends.csv\",encoding=\"UTF8\")\n",
    "            \n",
    "        # 키워드 빈도수 체크\n",
    "        def keyword_frequency_analysis(self,num=100,is_tfidf = True,is_noun=True, ma=\"twitter2\",prefix=\"\",s_words=[]):\n",
    "            \n",
    "            if ma == \"twitter\":\n",
    "                self.morph_analyzer = self.twitter\n",
    "            if ma == \"twitter2\":\n",
    "                self.morph_analyzer = self.twitter2    \n",
    "                \n",
    "            file_list = self.filtered_idx_result_df['file_list'].tolist()\n",
    "            print(len(file_list))\n",
    "            print(\"Keyword Analysis\")\n",
    "            corpus = [\"\"] * len(file_list)\n",
    "      \n",
    "            # 파일 내용 읽어서 코퍼스에 저장\n",
    "            process_time = time.time()\n",
    "            for i in range(len(file_list)):\n",
    "                if(i==1000):\n",
    "                    process_time = time.time() - process_time\n",
    "                    print(process_time)\n",
    "                print(i)\n",
    "                f = open(file_list[i], 'r',encoding='utf-8',errors=\"ignore\")\n",
    "                doc = f.read()\n",
    "                corpus[i] = \" \".join(self.morph_analyzer.nouns(doc))\n",
    "                \n",
    "            f.close()\n",
    "          \n",
    "            # Stop words \n",
    "            stop_words = ['광화문','기념','포항','화제','전북','남해','전남','유행','일보','마리','안전','오후','버스',\n",
    "              '관리','세상','경남','업계','하늘','우산','종로구','하경민','미디어','중부','기자회견','출근',\n",
    "              '공동','활짝','오전','남부','기술','청주','기사','진주','추진','날씨','집결','계획','대표',\n",
    "              '밀양시','장관','클릭','공사','전체','판단','전망','함양','관광객','여러분','영상','문의',\n",
    "              '지정','대체로','경향','억원','경북','지난','대회','제주도','공감','포항시','의원',\n",
    "              '사진','춘천','군청','평가','산천어','한국','한윤','스님','축제','기상청','최저','추위','동해',\n",
    "              '규모','행사','적극','함양군','호기','대책','경기','울산','기업','디지털','신문','일본',\n",
    "              '관계자','최고','코리아','피해','예상','시설','지구','미만','종원','복원','최신','미리','발견',\n",
    "              '회원','지방','예보','부산','제보','화천','만화','관련','총회','현상','성장','홍보','우리',\n",
    "              '사업','금지','기준','트렌드','광주','연구','환경','물결','제품','제공','무료','작품','참석자',\n",
    "              '제주','평년','강원','국제','가치나','가운데','농도','처리','연합','정책','세계','경우',\n",
    "              '조사','공단','대한','모바일','밀양','공원','지역','국내','주변','서울시','국회','전재',\n",
    "              '경기도','국가','강릉','위원회','정부','전주','올해','협의','초과','산업','차량','대강','언론',\n",
    "              '연합뉴스','인근','단체','발령','북부','정보','무단','조성','김용만','대해','아침','대구',\n",
    "              '결과','태안','안지','환경부','협악','먹이','전국','보이','지원','경향신문','주의보','건설', \n",
    "              '경제','자웅','위해','채용','배포','발효','독자','조금','개발','때문','일원','꽃망울','회의',\n",
    "              '차용','마을','수산','기상','협회','해수욕장','유엔','여수','사회','단지','유치','검사','구간',\n",
    "              '파란','바람','서해','해상','전시','창원','기름','새만금','반대','골프장','중단','동해안',\n",
    "              '장미','제도','인증','미투데이','바닷가','설악산','한라산','아파트','해경','충남','월드컵',\n",
    "              '완연','터트린','맑은','피어','만끽','청명','수준','소방','북상','영동','풍랑','영산강','환경청',\n",
    "              '일대','정비','꽃샘추위','계정','소통','미국','사람','한겨레','상승','보고서','투표','이자',\n",
    "              '여의도','윤중로','남산','설천면','서면','밀양아리랑','대전','충북','최강','주가','빅데이터',\n",
    "              '김선웅','이영환','학생','권역','춘분','스타벅스','커피','청계천','태백산','배훈식','김진아',\n",
    "              '페이스북','바로가기','트위터','몽골','아들','해제','처분','기해','안성','환자','평균','재판',\n",
    "              '캠페인','배출','사용','점검','채널','분리','설정','활동','영향','보통','이상','측정','상태', \n",
    "              '정취','낙동강','해체','환경운동','인사말','공급','원인','족구','근본','진도','계속','외모',\n",
    "              '업체','회관','상림','오늘','내일','시세','연구원','지난해','센터','대상','이번','사월','예정', \n",
    "              '가장','침대','타워','아래','종록','재촉','시간','파크','대저','스포츠','터트려','농촌진흥청',\n",
    "              '통해' '차관','이데일리','동물원','이천','경보','소독','인양','가을','장마전선','봄비','겨울',\n",
    "              '김은경','안병옥','이정선','여신','래시','명동','퍼포먼스','경찰','성분','영서','차차','빙어',\n",
    "              '세종','인천','함양읍','수원','서초구','중구','송파구','거창군','용인','남면','삼문동','매화',\n",
    "              '이나','통해','에서','잡고','맺힌','뽐내','채팅','맨손','잡기','평창동계올림픽','긴급','유채꽃',\n",
    "              '구매','현장','구독','생활','류형','포토','메인','단속','소식','공항','제주시','개막','물질',\n",
    "              '인제','남구','태백','나들이','강진','김태식','동구','화천군','낚시','우장','속초','사육',\n",
    "              '개선','시행','문제','차관','방안','주무관','무술년','새해','상춘객','공모전','현재','의성',\n",
    "              '화천읍','낭만','비상','뿌옇','출근길','도로','대부분','앞바다','조치','매우','고성','차바',\n",
    "              '새벽','밤새','일부','전날','군산','중심','맑음','주말','보가','임태훈','기록','가족','박주성',\n",
    "              '체험','청사','발생','산림청','설치','최근','협력','대응','날인','자태','인제군','마지막',\n",
    "              '식목일','서귀포시','영등포구','성남시','고승민','발걸음','고범준','부제','치가','사거리','거리',\n",
    "              '롯데','주위','모습','월드','용산구','망신','이하늬','세영','야구','콜라','가드','광장',\n",
    "              '유혹','첫날','탐방','자락','세종시','맞이','패럴림픽','하루','이동면','경칩','성남','입춘',\n",
    "              '기승','보기','협약','기관','운영','수거','작업','조명','장병','주차장','올림픽','운항','빗방울',\n",
    "              '백승','곳곳','방역','하동','하동군','연휴','추석','황금','배병수','홍천','평창','과장',\n",
    "              '휴식','강릉시','유형','화창','동진','최진','사이','포럼','개최','조경규','국립',\n",
    "              '운세','후회','여기','직원','물든','대통령','호텔','바닥','북구','청와대','추연','유해',\n",
    "              '왼쪽','시장','공연','주제','오른쪽','수원시','촬영','주년','정원','청소','현지','여명','강남구',\n",
    "              '판매','통합','종합','네이버','지청','당분간','추상','햇살','연일','풍경','흥부가','위로',\n",
    "              '이틀','한경닷컴','유승민','무소속','조현아','노출','총선','스폰서','비키니','치어리더',\n",
    "              '수확','개방','상황','본부','관측','스탠드','당부','푸른','정상','김경','고현면','고현면',\n",
    "              '오색','정유년','저녁','서울대','훈련','채취','권현구','김인철','이정섭','미녀','몸짱','에버랜드',\n",
    "              '매일','서비스','최창호','피해자','병원','저작권','적발','사고','남남서','건물','신고','국제공항',\n",
    "              '교육청','행동','행동','데이트','확률','가끔','점차','전해','강원도','일이','핫이슈','봄꽃',\n",
    "              '오픈','서비스','윤성규','성매매','과천','대한항공','몸매','동영상','봉사활동','유강','정선',\n",
    "              '후보','문호','업데이트','고속도로','태양','물놀이','의상','쯔위','드레스','학교','현황',\n",
    "              '남남서','동해','내륙','중부지방','분야','폭스바겐','문화','광장','주연','벗꽃','리우','지사',\n",
    "              '경주','경주시','글로벌','실시간','인턴','후보','특별','형산강','연일읍','북한','마트','가동',\n",
    "              '한경닷컴','박기량','교육','인사','빗물','안양','그룹','농림축산식품부','이상인','대기',\n",
    "              '교수','내년','운동장','사막','내외','가장자리','여객선','호인','누드','확산','파문','자료',\n",
    "              '아나운서','산간','절기','만원','수도권','강수','기상대','주의','람사르','지리산','발전',\n",
    "              '결항','산수유','북주','외국인','인산인해','주상','부평','둘째','서울시장','지하철','공공기관',\n",
    "              '자자체','촉구','동남아','새끼','학원','산지','인파','썰매','사무소','휴일','로봇','금은',\n",
    "              '우리나라','적용','권현구','가정','연휴','나무','설경','시청','나흘','우수','터널','목련',\n",
    "              '일출','개나리','기운','스카이','연일','성동구','산불','진화','낙시터','평택','매장','대란',\n",
    "              '동아일보','공기','라며','산책','강종민','축구','프레','심기','작용','카메라','사랑','부처님',\n",
    "              '송골송골','이물질','개양귀비','구경','바다','시민활동','동급','튤립','해양','모내기','안개',\n",
    "              '경기장','둥지','지방선거','철쭉','탄천','드론','서귀포','비행','소만','추억','다리','고기압',\n",
    "              '대진','고송민','거부','활약','시민행동','선언','이의','취소','형형색색','무상','잠실','운행',\n",
    "              '대기오염','임현정','협약','전망대','한강','식용','군락','환경재단','어린이집','구름','울릉도',\n",
    "              '연꽃','밀양강','전문가','모두','코스모스','마포구','봄날','대박','중앙','배경','꽁꽁','단풍',\n",
    "              '눈길','기온','영하','분수','계곡','농장','화성','교양','가득','먼지','단계','고양','고양시',\n",
    "              '화단','이형기','알피','대학교','성큼','선선','참가자','태현','광진구','평창군','최동준',\n",
    "              '처서','동민','서툴','해운대해수욕장','해운대','극심','본격','물고기','니스','대관령','상공',\n",
    "              '전신','서해안','길이','나들','소재','시내','독도','동선','인공','산란','공개','창원시',\n",
    "              '민주당','이진욱','방문','노동','물이','삼척','구조','통행','밤사이','안산','뉴시스',\n",
    "              '서울','벚꽃','뉴스','시민','기자','주민','국민','지속','지자체','북적','불법','최대',\n",
    "              '두번째','세종로','질환','이용','보고','박원순','뿌연','유튜브','모든','주시 반영','주시',\n",
    "              '반영','아름','더위','기고','아름 다운','도심','다운','국적','고명진','어가','지고','지나',\n",
    "              '어진','이루','알리','상암동','대공','시작','연출','부터','면서','부터','유의','다가',\n",
    "              '면서','린다','만들기','마련','전시회','다소','거나','아지','나타','제거','총장','참석',\n",
    "              '발길','화려','라보','피하','가운','사흠','국적','매달','연출','대공','기고','반영','부터 아지',\n",
    "              '다가 부터','부터 부터','수요일','화요일','월요일','목요일','금요일','토요일','일요일','유의 바란',\n",
    "              '박원순','온라인','공식','신청','진행','활용','정도','설명','기간','이후','다른',\n",
    "              '발표','주장','지적','우려','가능성','정도','요구','과정','한편','동안','다시','상부',\n",
    "              '이어진','온라인','공식','우려','처음','이유','다시','정도','요구','크게','사실',\n",
    "              '기간','포함','다음','화보','한편','가능성','이후','설명','분석','지금','각각','발표',\n",
    "              '크게','동안','이후','처음','주요','내용','실시','어린이','확인','특보','비롯','분포',\n",
    "              '증가','중인','가량','아시아','이동','지역별','규칙','바로','이하'\n",
    "             ]\n",
    "            \n",
    "            stop_words = list(set(stop_words))\n",
    "            if(len(s_words) != 0):\n",
    "                stop_words = s_words\n",
    "            \n",
    "            vect = CountVectorizer(stop_words = stop_words,ngram_range=(1, 2),min_df=0.005,max_df=0.3)\n",
    "            #ocab = vectorizer.get_feature_names()\n",
    "            #print(\"Execution\")\n",
    "            X = vect.fit_transform(corpus)\n",
    "            if(is_tfidf == True):\n",
    "                X = TfidfTransformer().fit_transform(X)\n",
    "                prefix = prefix + \"norm_\"\n",
    "                \n",
    "            count = X.toarray().sum(axis=0)\n",
    "            idx = np.argsort(-count)\n",
    "            count = count[idx]\n",
    "\n",
    "            feature_name = np.array(vect.get_feature_names())[idx]\n",
    "            #plt.bar(range(len(count)), count)\n",
    "            #plt.show()\n",
    "\n",
    "            self.keyword_list = list(zip(feature_name[:num], count[:num]))\n",
    "            result = pd.DataFrame(self.keyword_list)\n",
    "            result.to_csv(\"./keyword_frequency_result/\" + prefix + \"keyword_frequency.csv\",encoding=\"UTF8\")            \n",
    "            \n",
    "        # 키워드 트랜드 분석\n",
    "        # norm = 년도별 정규화\n",
    "        # voca = \"분석할 단어 설정\"\n",
    "        def keyword_trend_analysis(self, prefix=\"\", norm=False, ma ='twitter2',voca = \"\"):\n",
    "            \n",
    "            if ma == \"twitter\":\n",
    "                self.morph_analyzer = self.twitter\n",
    "            if ma == \"twitter2\":\n",
    "                self.morph_analyzer = self.twitter2\n",
    "            \n",
    "            temp_df = self.filtered_idx_result_df\n",
    "            file_list = self.filtered_idx_result_df['file_list'].tolist()\n",
    "            \n",
    "            x = list()\n",
    "            \n",
    "            # 각 vocabulary에 대해서 수행\n",
    "            result = pd.DataFrame()\n",
    "            for v in voca:\n",
    "                #print(v)\n",
    "                fidx = []\n",
    "                didx = []\n",
    "                for i in range(len(file_list)):\n",
    "                    f = open(file_list[i], 'r',encoding='utf-8',errors=\"ignore\")\n",
    "                    doc = f.read()\n",
    "                    f.close()\n",
    "                    p = re.compile(v)\n",
    "                    ridx = p.search(doc)\n",
    "                    if(ridx == None):\n",
    "                        didx.append(i)\n",
    "                    else:\n",
    "                        fidx.append(i)\n",
    "                    \n",
    "                # 날짜 추출\n",
    "                dt_list = [self.filtered_idx_result_df['date'].tolist()[sidx] for sidx in fidx]\n",
    "                dt_cnt = collections.Counter(list(map(lambda x : x.split(\"-\")[0], dt_list)))\n",
    "            \n",
    "                x = list(dt_cnt.keys())\n",
    "                y = list(dt_cnt.values())\n",
    "                y = pd.DataFrame(y)\n",
    "                result = pd.concat([result,y],axis=1)\n",
    "                \n",
    "           # dt_list = self.filtered_idx_result_df['date'].tolist()   \n",
    "           # dt_cnt = collections.Counter(list(map(lambda x : x.split(\"-\")[0], dt_list)))        \n",
    "            \n",
    "            result.columns = voca\n",
    "\n",
    "            if(norm == True):\n",
    "                prefix = \"norm_\" + prefix\n",
    "                temp_df_list = self.filtered_idx_result_df['date'].tolist()\n",
    "                all_dt_cnt = collections.Counter(list(map(lambda x : x.split(\"-\")[0], temp_df_list)))\n",
    "                \n",
    "                all_y = list(all_dt_cnt.values())\n",
    "                for i in range(len(all_y)):\n",
    "                    #print(result)\n",
    "                    result.iloc[i,:] = result.iloc[i,:] / all_y[i]\n",
    "            \n",
    "            x = pd.DataFrame(x)\n",
    "            result = pd.concat([x,result],axis=1)\n",
    "                        \n",
    "            result.to_csv(\"./keyword_trend_result/\" + prefix + \"keyword_trends.csv\",encoding=\"UTF8\")\n",
    "    \n",
    "        # 토픽 모델링\n",
    "        def topic_modeling(self,n_topics, ma = \"twitter2\", prefix = \"\",random_seed=1,s_words=[]):\n",
    "            \n",
    "            if ma == \"twitter\":\n",
    "                self.morph_analyzer = self.twitter\n",
    "            if ma == \"twitter2\":\n",
    "                self.morph_analyzer = self.twitter2\n",
    "                       \n",
    "            # 형태소 분석을 기본적으로 수행\n",
    "            file_list = self.filtered_idx_result_df['file_list'].tolist()\n",
    "            #print(file_list)\n",
    "            corpus = [\"\"] * len(file_list)\n",
    "\n",
    "            # 파일 내용 읽어서 코퍼스에 저장\n",
    "\n",
    "            for i in range(len(file_list)):\n",
    "                #print(i)\n",
    "                f = open(file_list[i], 'r',encoding='utf-8',errors=\"ignore\")\n",
    "                doc = f.read()\n",
    "                f.close()\n",
    "                corpus[i] = \" \".join(self.morph_analyzer.nouns(doc))\n",
    "\n",
    "            #print(corpus)\n",
    "\n",
    "            # Stop words ,\n",
    "     \n",
    "            stop_words = ['광화문','기념','포항','화제','전북','남해','전남','유행','일보','마리','안전','오후','버스',\n",
    "              '관리','세상','경남','업계','하늘','우산','종로구','하경민','미디어','중부','기자회견','출근',\n",
    "              '공동','활짝','오전','남부','기술','청주','기사','진주','추진','날씨','집결','계획','대표',\n",
    "              '밀양시','장관','클릭','공사','전체','판단','전망','함양','관광객','여러분','영상','문의',\n",
    "              '지정','대체로','녹색','경향','억원','경북','지난','대회','제주도','공감','포항시','의원',\n",
    "              '사진','춘천','군청','평가','산천어','한국','한윤','스님','축제','기상청','최저','추위','동해',\n",
    "              '규모','행사','적극','함양군','호기','대책','경기','울산','기업','디지털','신문','일본',\n",
    "              '관계자','최고','코리아','피해','예상','시설','지구','미만','종원','복원','최신','미리','발견',\n",
    "              '회원','지방','예보','부산','제보','화천','만화','관련','총회','현상','성장','홍보','우리',\n",
    "              '사업','금지','기준','트렌드','광주','연구','환경','물결','제품','제공','무료','작품','참석자',\n",
    "              '제주','평년','강원','국제','가치나','가운데','농도','처리','연합','정책','세계','경우',\n",
    "              '조사','공단','대한','모바일','밀양','공원','지역','국내','주변','서울시','국회','전재',\n",
    "              '경기도','국가','강릉','위원회','정부','전주','올해','협의','초과','산업','차량','대강','언론',\n",
    "              '연합뉴스','인근','단체','발령','북부','정보','무단','조성','김용만','대해','아침','대구',\n",
    "              '결과','태안','안지','환경부','협악','먹이','전국','보이','지원','경향신문','주의보','건설', \n",
    "              '경제','자웅','위해','채용','배포','발효','독자','조금','개발','때문','일원','꽃망울','회의',\n",
    "              '차용','마을','수산','기상','협회','해수욕장','유엔','여수','사회','단지','유치','검사','구간',\n",
    "              '파란','바람','서해','해상','전시','창원','기름','새만금','반대','골프장','중단','동해안',\n",
    "              '장미','제도','인증','미투데이','바닷가','설악산','한라산','아파트','해경','충남','월드컵',\n",
    "              '완연','터트린','맑은','피어','만끽','청명','수준','소방','북상','영동','풍랑','영산강','환경청',\n",
    "              '일대','정비','꽃샘추위','계정','소통','미국','사람','한겨레','상승','보고서','투표','이자',\n",
    "              '여의도','윤중로','남산','설천면','서면','밀양아리랑','대전','충북','최강','주가','빅데이터',\n",
    "              '김선웅','이영환','학생','권역','춘분','스타벅스','커피','청계천','태백산','배훈식','김진아',\n",
    "              '페이스북','바로가기','트위터','몽골','아들','해제','처분','기해','안성','환자','평균','재판',\n",
    "              '캠페인','배출','사용','점검','채널','분리','설정','활동','영향','보통','이상','측정','상태', \n",
    "              '정취','낙동강','해체','환경운동','인사말','공급','원인','족구','근본','진도','계속','외모',\n",
    "              '업체','회관','상림','오늘','내일','시세','연구원','지난해','센터','대상','이번','사월','예정', \n",
    "              '가장','침대','타워','아래','종록','재촉','시간','파크','대저','스포츠','터트려','농촌진흥청',\n",
    "              '통해' '차관','이데일리','동물원','이천','경보','소독','인양','가을','장마전선','봄비','겨울',\n",
    "              '김은경','안병옥','이정선','여신','래시','명동','퍼포먼스','경찰','성분','영서','차차','빙어',\n",
    "              '세종','인천','함양읍','수원','서초구','중구','송파구','거창군','용인','남면','삼문동','매화',\n",
    "              '이나','통해','에서','잡고','맺힌','뽐내','채팅','맨손','잡기','평창동계올림픽','긴급','유채꽃',\n",
    "              '구매','현장','구독','생활','류형','포토','메인','단속','소식','공항','제주시','개막','물질',\n",
    "              '인제','남구','태백','나들이','강진','김태식','동구','화천군','낚시','우장','속초','사육',\n",
    "              '개선','시행','문제','차관','방안','주무관','무술년','새해','상춘객','공모전','현재','의성',\n",
    "              '화천읍','낭만','비상','뿌옇','출근길','도로','대부분','앞바다','조치','매우','고성','차바',\n",
    "              '새벽','밤새','일부','전날','군산','중심','맑음','주말','보가','임태훈','기록','가족','박주성',\n",
    "              '체험','청사','발생','산림청','설치','최근','협력','대응','날인','자태','인제군','마지막',\n",
    "              '식목일','서귀포시','영등포구','성남시','고승민','발걸음','고범준','부제','치가','사거리','거리',\n",
    "              '롯데','주위','모습','월드','용산구','망신','이하늬','세영','야구','콜라','가드','광장',\n",
    "              '유혹','첫날','탐방','자락','세종시','맞이','패럴림픽','하루','이동면','경칩','성남','입춘',\n",
    "              '기승','보기','협약','기관','운영','수거','작업','조명','장병','주차장','올림픽','운항','빗방울',\n",
    "              '백승','곳곳','방역','하동','하동군','연휴','추석','황금','배병수','홍천','평창','과장',\n",
    "              '휴식','강릉시','유형','화창','동진','최진','사이','포럼','개최','조경규','국립',\n",
    "              '운세','후회','여기','직원','물든','대통령','호텔','바닥','북구','청와대','추연','유해',\n",
    "              '왼쪽','시장','공연','주제','오른쪽','수원시','촬영','주년','정원','청소','현지','여명','강남구',\n",
    "              '판매','통합','종합','네이버','지청','당분간','추상','햇살','연일','풍경','흥부가','위로',\n",
    "              '이틀','한경닷컴','유승민','무소속','조현아','노출','총선','스폰서','비키니','치어리더',\n",
    "              '수확','개방','상황','본부','관측','스탠드','당부','푸른','정상','김경','고현면','고현면',\n",
    "              '오색','정유년','저녁','서울대','훈련','채취','권현구','김인철','이정섭','미녀','몸짱','에버랜드',\n",
    "              '매일','서비스','최창호','피해자','병원','저작권','적발','사고','남남서','건물','신고','국제공항',\n",
    "              '교육청','행동','행동','데이트','확률','가끔','점차','전해','강원도','일이','핫이슈w','봄꽃',\n",
    "              '오픈','서비스','윤성규','성매매','과천','대한항공','몸매','동영상','봉사활동','유강','정선',\n",
    "              '후보','문호','업데이트','고속도로','태양','물놀이','의상','쯔위','드레스','학교','현황',\n",
    "              '남남서','동해','내륙','중부지방','분야','폭스바겐','문화','광장','주연','벗꽃','리우','지사',\n",
    "              '경주','경주시','글로벌','실시간','인턴','후보','특별','형산강','연일읍','북한','마트','가동',\n",
    "              '한경닷컴','박기량','교육','인사','빗물','안양','그룹','농림축산식품부','이상인','대기',\n",
    "              '교수','내년','운동장','사막','내외','가장자리','여객선','호인','누드','확산','파문','자료',\n",
    "              '아나운서','산간','절기','만원','수도권','강수','기상대','주의','람사르','지리산','발전',\n",
    "              '결항','산수유','북주','외국인','인산인해','주상','부평','둘째','서울시장','지하철','공공기관',\n",
    "              '자자체','촉구','동남아','새끼','학원','산지','인파','썰매','사무소','휴일','로봇','금은',\n",
    "              '우리나라','적용','권현구','가정','연휴','나무','설경','시청','나흘','우수','터널','목련',\n",
    "              '일출','개나리','기운','스카이','연일','성동구','산불','진화','낙시터','평택','매장','대란',\n",
    "              '동아일보','공기','라며','산책','강종민','축구','프레','심기','작용','카메라','사랑','부처님',\n",
    "              '송골송골','이물질','개양귀비','구경','바다','시민활동','동급','튤립','해양','모내기','안개',\n",
    "              '경기장','둥지','지방선거','철쭉','탄천','드론','서귀포','비행','소만','추억','다리','고기압',\n",
    "              '대진','고송민','거부','활약','시민행동','선언','이의','취소','형형색색','무상','잠실','운행',\n",
    "              '대기오염','임현정','협약','전망대','한강','식용','군락','환경재단','어린이집','구름','울릉도',\n",
    "              '연꽃','밀양강','전문가','모두','코스모스','마포구','봄날','대박','중앙','배경','꽁꽁','단풍',\n",
    "              '눈길','기온','영하','분수','계곡','농장','화성','교양','가득','먼지','단계','고양','고양시',\n",
    "              '화단','이형기','알피','대학교','성큼','선선','참가자','태현','광진구','평창군','최동준',\n",
    "              '처서','동민','서툴','해운대해수욕장','해운대','극심','본격','물고기','니스','대관령','상공',\n",
    "              '전신','서해안','길이','나들','소재','시내','독도','동선','인공','산란','공개','창원시',\n",
    "              '민주당','이진욱','방문','노동','물이','삼척','구조','통행','밤사이','안산','뉴시스',\n",
    "              '서울','벚꽃','뉴스','시민','기자','주민','국민','지속','지자체','북적','불법','최대',\n",
    "              '두번째','세종로','질환','이용','보고','박원순','뿌연','유튜브','모든','주시 반영','주시',\n",
    "              '반영','아름','더위','기고','아름 다운','도심','다운','국적','고명진','어가','지고','지나',\n",
    "              '어진','이루','알리','상암동','대공','시작','연출','부터','면서','부터','유의','다가',\n",
    "              '면서','린다','만들기','마련','전시회','다소','거나','아지','나타','제거','총장','참석',\n",
    "              '발길','화려','라보','피하','가운','사흠','국적','매달','연출','대공','기고','반영','부터 아지',\n",
    "              '다가 부터','부터 부터','수요일','화요일','월요일','목요일','금요일','토요일','일요일','유의 바란',\n",
    "              '박원순','온라인','공식','신청','진행','활용','정도','설명','기간','이후','다른',\n",
    "              '발표','주장','지적','우려','가능성','정도','요구','과정','한편','동안','다시','상부',\n",
    "              '이어진','온라인','공식','우려','처음','이유','다시','정도','요구','크게','사실',\n",
    "              '기간','포함','다음','화보','한편','가능성','이후','설명','분석','지금','각각','발표',\n",
    "              '크게','동안','이후','처음','주요','내용','실시'                 \n",
    "             ]\n",
    "        \n",
    "            stop_words = list(set(stop_words))\n",
    "            if(len(s_words) != 0):\n",
    "                stop_words = s_words\n",
    "            \n",
    "            vect = CountVectorizer(stop_words = stop_words,ngram_range=(1, 2),min_df=0.005,max_df=0.3)\n",
    "            X = vect.fit_transform(corpus)\n",
    "            X = TfidfTransformer().fit_transform(X)\n",
    "\n",
    "            #vect = CountVectorizer(stop_words = stop_words,min_df=0.005,max_df=0.3)\n",
    "\n",
    "            n_words = 300\n",
    "            for i in n_topics:\n",
    "                print(i)\n",
    "                # LDA 수횅\n",
    "                lda = LatentDirichletAllocation(n_components = i,learning_method=\"batch\",random_state=random_seed)\n",
    "                #lda = LatentDirichletAllocation(learning_method=\"batch\")\n",
    "                self.document_topics = lda.fit_transform(X)\n",
    "                \n",
    "                sorting = np.argsort(lda.components_, axis=1)[:,::-1]\n",
    "                feature_names = np.array(vect.get_feature_names())\n",
    "                \n",
    "                #mglearn.tools.print_topics(topics=range(i), feature_names = feature_names, sorting=sorting, topics_per_chunk=20, n_words=20)\n",
    "                #self.filtered_idx_result_df[\"topic\"] = np.argmax(self.document_topics,axis=1).tolist()\n",
    "                \n",
    "                self.topic_list = np.argmax(self.document_topics,axis=1).tolist()\n",
    "                \n",
    "                self.topic_words = pd.DataFrame(feature_names[sorting])\n",
    "                self.topic_words = self.topic_words.T\n",
    "                self.topic_words = self.topic_words[:n_words]\n",
    "                \n",
    "                # 파일이름 지정\n",
    "                filename =  \"./lda_result/\" +  str(prefix) + \"_result_n_\" + str(i) + \"_\" \n",
    "\n",
    "                # 파일 저장\n",
    "                self.document_topics = pd.DataFrame(self.document_topics)\n",
    "                self.document_topics.to_csv(filename + \"document_topic.csv\",header=None,index=False,encoding=\"UTF8\")\n",
    "                self.topic_words.to_csv(filename + \"topic_words.csv\",header=None,index=False,encoding=\"UTF8\")\n",
    "                \n",
    "                # 시각화 및 저장\n",
    "                self.topic_modeling_vis(lda, X, vect, filename)\n",
    "\n",
    "        def topic_modeling_vis(self,lda,dtm,vect,filename):\n",
    "            pyldavis = pyLDAvis.sklearn.prepare(lda, dtm,vect)\n",
    "            pyLDAvis.save_html(pyldavis,filename + \"lda.html\")\n",
    "            \n",
    "        def select_topics(self,topics):\n",
    "            #print(\"Select Topics\")\n",
    "            topic_list = self.topic_list\n",
    "            #print(topic_list)\n",
    "            tidx = list(map(lambda x: x in topics , topic_list))\n",
    "            #print(tidx)\n",
    "            \n",
    "            self.filtered_idx_result_df = self.filtered_idx_result_df.iloc[tidx,:]\n",
    "            self.filtered_idx_result_df = self.filtered_idx_result_df.reset_index(drop=True)\n",
    "            self.topic_list = np.array(self.topic_list)[tidx].tolist()\n",
    "            \n",
    "        def topic_modeling_statistics(self,prefix=\"\"):\n",
    "            \n",
    "            topic_list = self.topic_list\n",
    "            result = pd.DataFrame()\n",
    "\n",
    "            dt = None\n",
    "            for t in range(max(topic_list) + 1):\n",
    "                print(t)\n",
    "                sub_idx = (np.array(topic_list) == t)\n",
    "                sub_idx = list(sub_idx)\n",
    "                sub_result = self.filtered_idx_result_df.loc[sub_idx,:]\n",
    "                date_list = sub_result['date']\n",
    "                all_dt_cnt = collections.Counter(list(map(lambda x : x.split(\"-\")[0], date_list)))\n",
    "                all_y = list(all_dt_cnt.values())\n",
    "                all_y = pd.DataFrame(all_y)\n",
    "                result = pd.concat([result,all_y],axis=1)            \n",
    "\n",
    "            dt = list(all_dt_cnt.keys())\n",
    "            dt = pd.DataFrame(dt)\n",
    "            result = pd.concat([dt,result],axis=1)\n",
    "\n",
    "            result.to_csv(\"./lda_stat/\" + prefix + \"lda_stat.csv\",encoding=\"UTF8\")\n",
    "        \n",
    "            \n",
    "        def extract_sentence(self,rexp,ma=\"twitter2\",prefix=\"\"):\n",
    "            w1 = rexp\n",
    "            if ma == \"twitter\":\n",
    "                self.morph_analyzer = self.twitter\n",
    "            if ma == \"twitter2\":\n",
    "                self.morph_analyzer = self.twitter2  \n",
    "\n",
    "            file_list =  self.filtered_idx_result_df['file_list'].tolist()\n",
    "            filtered_sentences= []\n",
    "            for i in range(len(file_list)):\n",
    "                f = open(file_list[i], 'r',encoding='utf-8',errors=\"ignore\")\n",
    "                doc = f.read()\n",
    "                f.close()\n",
    "                sentences = self.split_text(doc)\n",
    "                \n",
    "                for s in sentences:\n",
    "                    filtered_sentences.append(s)\n",
    "            \n",
    "            self.temp = pd.DataFrame(filtered_sentences)\n",
    "            \n",
    "            for w in w1:\n",
    "                temp = []\n",
    "                for s in filtered_sentences :\n",
    "                    p = re.compile(w)\n",
    "                    ridx = p.search(s)\n",
    "\n",
    "                    if(ridx == None):\n",
    "                       #print(\"없음\")\n",
    "                       continue\n",
    "                        \n",
    "                    s = s.strip()\n",
    "                    #s = re.sub('[^가-힝0-9a-zA-Z\\\\s]', '', s)\n",
    "                    temp.append(s)\n",
    "                filtered_sentences = temp\n",
    "            \n",
    "            self.filtered_sentences = filtered_sentences\n",
    "            result = pd.DataFrame(self.filtered_sentences)\n",
    "            \n",
    "            result.to_csv(\"./filtered_sentences_result/\" + prefix + \"filtered_sentences.csv\")\n",
    "            \n",
    "        # word2vec 학습\n",
    "        def training_keyword_w2v(self,ma=\"twitter2\"):\n",
    "            file_list = self.filtered_idx_result_df['file_list'].tolist()\n",
    "            corpus = [\"\"] * len(file_list)\n",
    "            # 형태소 분석기 선택\n",
    "\n",
    "            if ma == \"twitter\":\n",
    "                self.morph_analyzer = self.twitter\n",
    "            if ma == \"twitter2\":\n",
    "                self.morph_analyzer = self.twitter2\n",
    "                \n",
    "            # 파일 내용 읽어서 코퍼스에 저장\n",
    "            for i in range(len(file_list)):\n",
    "                #print(i)\n",
    "                f = open(file_list[i], 'r',encoding='utf-8',errors=\"ignore\")\n",
    "                doc = f.read()\n",
    "                \n",
    "                doc = re.sub('[^가-힝0-9a-zA-Z\\\\s]', '', doc)\n",
    "                corpus[i] = self.morph_analyzer.nouns(doc)\n",
    "\n",
    "                \n",
    "                \n",
    "            model = Word2Vec(corpus)\n",
    "            model.init_sims(replace=True)\n",
    "            self.w2v_model = model\n",
    "        \n",
    "        # word2vec 학습 결과 저장\n",
    "        def w2v_analysis(self,word,num=100):\n",
    "            #print(self.w2v_model.most_similatr(word,topn=num))\n",
    "            result = pd.DataFrame(self.w2v_model.most_similar(word,topn=num))\n",
    "            result2 = pd.DataFrame(self.w2v_model.predict_output_word(word, topn=num))\n",
    "            filename = \"\".join(word)\n",
    "            result.to_csv(\"./w2v_result/\" + \"w2v_\" + filename + \".csv\",encoding=\"UTF8\")\n",
    "            result.to_csv(\"./w2v_result/\" + \"w2v2_\" + filename + \".csv\",encoding=\"UTF8\")\n",
    "       \n",
    "        # 문서 레벨 동시 출현 단어\n",
    "        def doc_level_coccurrence_words(self,w1, num=100,ma=\"twitter2\",prefix=\"\",s_words=[]):\n",
    "            if ma == \"twitter\":\n",
    "                self.morph_analyzer = self.twitter\n",
    "            if ma == \"twitter2\":\n",
    "                self.morph_analyzer = self.twitter2  \n",
    "                \n",
    "            # 문서추출\n",
    "            file_list =  self.filtered_idx_result_df['file_list'].tolist()\n",
    "            temp_idx_result_df = pd.DataFrame()\n",
    "            #print(file_list)\n",
    "            \n",
    "            fidx = []\n",
    "            didx = []\n",
    "            for w in w1 :\n",
    "                for i in range(len(file_list)):\n",
    "                    f = open(file_list[i], 'r',encoding='utf-8',errors=\"ignore\")\n",
    "                    doc = f.read()\n",
    "                    p = re.compile(w)\n",
    "                    ridx = p.search(doc)\n",
    "\n",
    "                    if(ridx == None):\n",
    "                       #print(\"없음\")\n",
    "                        didx.append(i)\n",
    "\n",
    "                    else:\n",
    "                        fidx.append(i)\n",
    "                        \n",
    "            fidx = list(set(fidx))\n",
    "            #print(fidx)\n",
    "            temp_idx_result_df = self.filtered_idx_result_df.iloc[fidx]\n",
    "            temp_idx_result_df = temp_idx_result_df.reset_index(drop=True)\n",
    "            \n",
    "            file_list =  temp_idx_result_df['file_list'].tolist()\n",
    "            \n",
    "            corpus = []     \n",
    "            for i in range(len(file_list)):\n",
    "                f = open(file_list[i], 'r',encoding='utf-8',errors=\"ignore\")\n",
    "                doc = f.read()\n",
    "                \n",
    "                # 파일 내용 읽어서 코퍼스에 저장\n",
    "                corpus.append(\" \".join(self.morph_analyzer.nouns(doc)))\n",
    "\n",
    "                      # Stop words \n",
    "            stop_words = ['광화문','기념','포항','화제','전북','남해','전남','유행','일보','마리','안전','오후','버스',\n",
    "              '관리','세상','경남','업계','하늘','우산','종로구','하경민','미디어','중부','기자회견','출근',\n",
    "              '공동','활짝','오전','남부','기술','청주','기사','진주','추진','날씨','집결','계획','대표',\n",
    "              '밀양시','장관','클릭','공사','전체','판단','전망','함양','관광객','여러분','영상','문의',\n",
    "              '지정','대체로','경향','억원','경북','지난','대회','제주도','공감','포항시','의원',\n",
    "              '사진','춘천','군청','평가','산천어','한국','한윤','스님','축제','기상청','최저','추위','동해',\n",
    "              '규모','행사','적극','함양군','호기','대책','경기','울산','기업','디지털','신문','일본',\n",
    "              '관계자','최고','코리아','피해','예상','시설','지구','미만','종원','복원','최신','미리','발견',\n",
    "              '회원','지방','예보','부산','제보','화천','만화','관련','총회','현상','성장','홍보','우리',\n",
    "              '사업','금지','기준','트렌드','광주','연구','환경','물결','제품','제공','무료','작품','참석자',\n",
    "              '제주','평년','강원','국제','가치나','가운데','농도','처리','연합','정책','세계','경우',\n",
    "              '조사','공단','대한','모바일','밀양','공원','지역','국내','주변','서울시','국회','전재',\n",
    "              '경기도','국가','강릉','위원회','정부','전주','올해','협의','초과','산업','차량','대강','언론',\n",
    "              '연합뉴스','인근','단체','발령','북부','정보','무단','조성','김용만','대해','아침','대구',\n",
    "              '결과','태안','안지','환경부','협악','먹이','전국','보이','지원','경향신문','주의보','건설', \n",
    "              '경제','자웅','위해','채용','배포','발효','독자','조금','개발','때문','일원','꽃망울','회의',\n",
    "              '차용','마을','수산','기상','협회','해수욕장','유엔','여수','사회','단지','유치','검사','구간',\n",
    "              '파란','바람','서해','해상','전시','창원','기름','새만금','반대','골프장','중단','동해안',\n",
    "              '장미','제도','인증','미투데이','바닷가','설악산','한라산','아파트','해경','충남','월드컵',\n",
    "              '완연','터트린','맑은','피어','만끽','청명','수준','소방','북상','영동','풍랑','영산강','환경청',\n",
    "              '일대','정비','꽃샘추위','계정','소통','미국','사람','한겨레','상승','보고서','투표','이자',\n",
    "              '여의도','윤중로','남산','설천면','서면','밀양아리랑','대전','충북','최강','주가','빅데이터',\n",
    "              '김선웅','이영환','학생','권역','춘분','스타벅스','커피','청계천','태백산','배훈식','김진아',\n",
    "              '페이스북','바로가기','트위터','몽골','아들','해제','처분','기해','안성','환자','평균','재판',\n",
    "              '캠페인','배출','사용','점검','채널','분리','설정','활동','영향','보통','이상','측정','상태', \n",
    "              '정취','낙동강','해체','환경운동','인사말','공급','원인','족구','근본','진도','계속','외모',\n",
    "              '업체','회관','상림','오늘','내일','시세','연구원','지난해','센터','대상','이번','사월','예정', \n",
    "              '가장','침대','타워','아래','종록','재촉','시간','파크','대저','스포츠','터트려','농촌진흥청',\n",
    "              '통해' '차관','이데일리','동물원','이천','경보','소독','인양','가을','장마전선','봄비','겨울',\n",
    "              '김은경','안병옥','이정선','여신','래시','명동','퍼포먼스','경찰','성분','영서','차차','빙어',\n",
    "              '세종','인천','함양읍','수원','서초구','중구','송파구','거창군','용인','남면','삼문동','매화',\n",
    "              '이나','통해','에서','잡고','맺힌','뽐내','채팅','맨손','잡기','평창동계올림픽','긴급','유채꽃',\n",
    "              '구매','현장','구독','생활','류형','포토','메인','단속','소식','공항','제주시','개막','물질',\n",
    "              '인제','남구','태백','나들이','강진','김태식','동구','화천군','낚시','우장','속초','사육',\n",
    "              '개선','시행','문제','차관','방안','주무관','무술년','새해','상춘객','공모전','현재','의성',\n",
    "              '화천읍','낭만','비상','뿌옇','출근길','도로','대부분','앞바다','조치','매우','고성','차바',\n",
    "              '새벽','밤새','일부','전날','군산','중심','맑음','주말','보가','임태훈','기록','가족','박주성',\n",
    "              '체험','청사','발생','산림청','설치','최근','협력','대응','날인','자태','인제군','마지막',\n",
    "              '식목일','서귀포시','영등포구','성남시','고승민','발걸음','고범준','부제','치가','사거리','거리',\n",
    "              '롯데','주위','모습','월드','용산구','망신','이하늬','세영','야구','콜라','가드','광장',\n",
    "              '유혹','첫날','탐방','자락','세종시','맞이','패럴림픽','하루','이동면','경칩','성남','입춘',\n",
    "              '기승','보기','협약','기관','운영','수거','작업','조명','장병','주차장','올림픽','운항','빗방울',\n",
    "              '백승','곳곳','방역','하동','하동군','연휴','추석','황금','배병수','홍천','평창','과장',\n",
    "              '휴식','강릉시','유형','화창','동진','최진','사이','포럼','개최','조경규','국립',\n",
    "              '운세','후회','여기','직원','물든','대통령','호텔','바닥','북구','청와대','추연','유해',\n",
    "              '왼쪽','시장','공연','주제','오른쪽','수원시','촬영','주년','정원','청소','현지','여명','강남구',\n",
    "              '판매','통합','종합','네이버','지청','당분간','추상','햇살','연일','풍경','흥부가','위로',\n",
    "              '이틀','한경닷컴','유승민','무소속','조현아','노출','총선','스폰서','비키니','치어리더',\n",
    "              '수확','개방','상황','본부','관측','스탠드','당부','푸른','정상','김경','고현면','고현면',\n",
    "              '오색','정유년','저녁','서울대','훈련','채취','권현구','김인철','이정섭','미녀','몸짱','에버랜드',\n",
    "              '매일','서비스','최창호','피해자','병원','저작권','적발','사고','남남서','건물','신고','국제공항',\n",
    "              '교육청','행동','행동','데이트','확률','가끔','점차','전해','강원도','일이','핫이슈','봄꽃',\n",
    "              '오픈','서비스','윤성규','성매매','과천','대한항공','몸매','동영상','봉사활동','유강','정선',\n",
    "              '후보','문호','업데이트','고속도로','태양','물놀이','의상','쯔위','드레스','학교','현황',\n",
    "              '남남서','동해','내륙','중부지방','분야','폭스바겐','문화','광장','주연','벗꽃','리우','지사',\n",
    "              '경주','경주시','글로벌','실시간','인턴','후보','특별','형산강','연일읍','북한','마트','가동',\n",
    "              '한경닷컴','박기량','교육','인사','빗물','안양','그룹','농림축산식품부','이상인','대기',\n",
    "              '교수','내년','운동장','사막','내외','가장자리','여객선','호인','누드','확산','파문','자료',\n",
    "              '아나운서','산간','절기','만원','수도권','강수','기상대','주의','람사르','지리산','발전',\n",
    "              '결항','산수유','북주','외국인','인산인해','주상','부평','둘째','서울시장','지하철','공공기관',\n",
    "              '자자체','촉구','동남아','새끼','학원','산지','인파','썰매','사무소','휴일','로봇','금은',\n",
    "              '우리나라','적용','권현구','가정','연휴','나무','설경','시청','나흘','우수','터널','목련',\n",
    "              '일출','개나리','기운','스카이','연일','성동구','산불','진화','낙시터','평택','매장','대란',\n",
    "              '동아일보','공기','라며','산책','강종민','축구','프레','심기','작용','카메라','사랑','부처님',\n",
    "              '송골송골','이물질','개양귀비','구경','바다','시민활동','동급','튤립','해양','모내기','안개',\n",
    "              '경기장','둥지','지방선거','철쭉','탄천','드론','서귀포','비행','소만','추억','다리','고기압',\n",
    "              '대진','고송민','거부','활약','시민행동','선언','이의','취소','형형색색','무상','잠실','운행',\n",
    "              '대기오염','임현정','협약','전망대','한강','식용','군락','환경재단','어린이집','구름','울릉도',\n",
    "              '연꽃','밀양강','전문가','모두','코스모스','마포구','봄날','대박','중앙','배경','꽁꽁','단풍',\n",
    "              '눈길','기온','영하','분수','계곡','농장','화성','교양','가득','먼지','단계','고양','고양시',\n",
    "              '화단','이형기','알피','대학교','성큼','선선','참가자','태현','광진구','평창군','최동준',\n",
    "              '처서','동민','서툴','해운대해수욕장','해운대','극심','본격','물고기','니스','대관령','상공',\n",
    "              '전신','서해안','길이','나들','소재','시내','독도','동선','인공','산란','공개','창원시',\n",
    "              '민주당','이진욱','방문','노동','물이','삼척','구조','통행','밤사이','안산','뉴시스',\n",
    "              '서울','벚꽃','뉴스','시민','기자','주민','국민','지속','지자체','북적','불법','최대',\n",
    "              '두번째','세종로','질환','이용','보고','박원순','뿌연','유튜브','모든','주시 반영','주시',\n",
    "              '반영','아름','더위','기고','아름 다운','도심','다운','국적','고명진','어가','지고','지나',\n",
    "              '어진','이루','알리','상암동','대공','시작','연출','부터','면서','부터','유의','다가',\n",
    "              '면서','린다','만들기','마련','전시회','다소','거나','아지','나타','제거','총장','참석',\n",
    "              '발길','화려','라보','피하','가운','사흠','국적','매달','연출','대공','기고','반영','부터 아지',\n",
    "              '다가 부터','부터 부터','수요일','화요일','월요일','목요일','금요일','토요일','일요일','유의 바란',\n",
    "              '박원순','온라인','공식','신청','진행','활용','정도','설명','기간','이후','다른',\n",
    "              '발표','주장','지적','우려','가능성','정도','요구','과정','한편','동안','다시','상부',\n",
    "              '이어진','온라인','공식','우려','처음','이유','다시','정도','요구','크게','사실',\n",
    "              '기간','포함','다음','화보','한편','가능성','이후','설명','분석','지금','각각','발표',\n",
    "              '크게','동안','이후','처음','주요','내용','실시','어린이','확인','특보','비롯','분포',\n",
    "              '증가','중인','가량','아시아','이동','지역별','규칙','바로','이하'\n",
    "             ]\n",
    "            \n",
    "            stop_words = list(set(stop_words))\n",
    "\n",
    "            if(len(s_words) != 0):\n",
    "                stop_words = s_words\n",
    "                \n",
    "            vect = CountVectorizer(stop_words = stop_words,ngram_range=(1, 2),min_df=0.005,max_df=0.3)\n",
    "       \n",
    "            X = vect.fit_transform(corpus)\n",
    "\n",
    "            count = X.toarray().sum(axis=0)\n",
    "            idx = np.argsort(-count)\n",
    "            count = count[idx]\n",
    "\n",
    "            feature_name = np.array(vect.get_feature_names())[idx]\n",
    "            #plt.bar(range(len(count)), count)\n",
    "            #plt.show()\n",
    "\n",
    "            co_word_list = list(zip(feature_name[:num], count[:num]))\n",
    "            co_word_list = pd.DataFrame(co_word_list)\n",
    "            filename = \"\".join(w1)\n",
    "            co_word_list.to_csv(\"./doc_level_cowords/\" + prefix + filename + \"_dcowords\" + \".csv\",encoding=\"UTF8\")\n",
    "   \n",
    "       # 문장레벨 동시 출현 단어\n",
    "        def sentence_level_coccurrence_words(self,w1, num=100, ma=\"twitter2\",prefix=\"\",s_words=[]):\n",
    "            \n",
    "            if ma == \"twitter\":\n",
    "                self.morph_analyzer = self.twitter\n",
    "            if ma == \"twitter2\":\n",
    "                self.morph_analyzer = self.twitter2  \n",
    "\n",
    "            file_list =  self.filtered_idx_result_df['file_list'].tolist()\n",
    "            filtered_sentences= []\n",
    "            for i in range(len(file_list)):\n",
    "                f = open(file_list[i], 'r',encoding='utf-8',errors=\"ignore\")\n",
    "                doc = f.read()\n",
    "                sentences = self.split_text(doc)\n",
    "                \n",
    "                for s in sentences:\n",
    "                    filtered_sentences.append(s)\n",
    "            \n",
    "            self.temp = pd.DataFrame(filtered_sentences)\n",
    "            \n",
    "            for w in w1:\n",
    "                temp = []\n",
    "                for s in filtered_sentences :\n",
    "                    p = re.compile(w)\n",
    "                    ridx = p.search(s)\n",
    "\n",
    "                    if(ridx == None):\n",
    "                        continue   \n",
    "                    s = s.strip()\n",
    "                    temp.append(s)\n",
    "                filtered_sentences = temp\n",
    "            \n",
    "            self.temp = pd.DataFrame(filtered_sentences)\n",
    "            print(self.temp)\n",
    "            # 파일 내용 읽어서 코퍼스에 저장\n",
    "    \n",
    "            corpus = []     \n",
    "            for i in range(len(filtered_sentences)):\n",
    "                corpus.append(\" \".join(self.morph_analyzer.nouns(filtered_sentences[i])))\n",
    "            \n",
    "                      # Stop words \n",
    "            stop_words = ['광화문','기념','포항','화제','전북','남해','전남','유행','일보','마리','안전','오후','버스',\n",
    "              '관리','세상','경남','업계','하늘','우산','종로구','하경민','미디어','중부','기자회견','출근',\n",
    "              '공동','활짝','오전','남부','기술','청주','기사','진주','추진','날씨','집결','계획','대표',\n",
    "              '밀양시','장관','클릭','공사','전체','판단','전망','함양','관광객','여러분','영상','문의',\n",
    "              '지정','대체로','경향','억원','경북','지난','대회','제주도','공감','포항시','의원',\n",
    "              '사진','춘천','군청','평가','산천어','한국','한윤','스님','축제','기상청','최저','추위','동해',\n",
    "              '규모','행사','적극','함양군','호기','대책','경기','울산','기업','디지털','신문','일본',\n",
    "              '관계자','최고','코리아','피해','예상','시설','지구','미만','종원','복원','최신','미리','발견',\n",
    "              '회원','지방','예보','부산','제보','화천','만화','관련','총회','현상','성장','홍보','우리',\n",
    "              '사업','금지','기준','트렌드','광주','연구','환경','물결','제품','제공','무료','작품','참석자',\n",
    "              '제주','평년','강원','국제','가치나','가운데','농도','처리','연합','정책','세계','경우',\n",
    "              '조사','공단','대한','모바일','밀양','공원','지역','국내','주변','서울시','국회','전재',\n",
    "              '경기도','국가','강릉','위원회','정부','전주','올해','협의','초과','산업','차량','대강','언론',\n",
    "              '연합뉴스','인근','단체','발령','북부','정보','무단','조성','김용만','대해','아침','대구',\n",
    "              '결과','태안','안지','환경부','협악','먹이','전국','보이','지원','경향신문','주의보','건설', \n",
    "              '경제','자웅','위해','채용','배포','발효','독자','조금','개발','때문','일원','꽃망울','회의',\n",
    "              '차용','마을','수산','기상','협회','해수욕장','유엔','여수','사회','단지','유치','검사','구간',\n",
    "              '파란','바람','서해','해상','전시','창원','기름','새만금','반대','골프장','중단','동해안',\n",
    "              '장미','제도','인증','미투데이','바닷가','설악산','한라산','아파트','해경','충남','월드컵',\n",
    "              '완연','터트린','맑은','피어','만끽','청명','수준','소방','북상','영동','풍랑','영산강','환경청',\n",
    "              '일대','정비','꽃샘추위','계정','소통','미국','사람','한겨레','상승','보고서','투표','이자',\n",
    "              '여의도','윤중로','남산','설천면','서면','밀양아리랑','대전','충북','최강','주가','빅데이터',\n",
    "              '김선웅','이영환','학생','권역','춘분','스타벅스','커피','청계천','태백산','배훈식','김진아',\n",
    "              '페이스북','바로가기','트위터','몽골','아들','해제','처분','기해','안성','환자','평균','재판',\n",
    "              '캠페인','배출','사용','점검','채널','분리','설정','활동','영향','보통','이상','측정','상태', \n",
    "              '정취','낙동강','해체','환경운동','인사말','공급','원인','족구','근본','진도','계속','외모',\n",
    "              '업체','회관','상림','오늘','내일','시세','연구원','지난해','센터','대상','이번','사월','예정', \n",
    "              '가장','침대','타워','아래','종록','재촉','시간','파크','대저','스포츠','터트려','농촌진흥청',\n",
    "              '통해' '차관','이데일리','동물원','이천','경보','소독','인양','가을','장마전선','봄비','겨울',\n",
    "              '김은경','안병옥','이정선','여신','래시','명동','퍼포먼스','경찰','성분','영서','차차','빙어',\n",
    "              '세종','인천','함양읍','수원','서초구','중구','송파구','거창군','용인','남면','삼문동','매화',\n",
    "              '이나','통해','에서','잡고','맺힌','뽐내','채팅','맨손','잡기','평창동계올림픽','긴급','유채꽃',\n",
    "              '구매','현장','구독','생활','류형','포토','메인','단속','소식','공항','제주시','개막','물질',\n",
    "              '인제','남구','태백','나들이','강진','김태식','동구','화천군','낚시','우장','속초','사육',\n",
    "              '개선','시행','문제','차관','방안','주무관','무술년','새해','상춘객','공모전','현재','의성',\n",
    "              '화천읍','낭만','비상','뿌옇','출근길','도로','대부분','앞바다','조치','매우','고성','차바',\n",
    "              '새벽','밤새','일부','전날','군산','중심','맑음','주말','보가','임태훈','기록','가족','박주성',\n",
    "              '체험','청사','발생','산림청','설치','최근','협력','대응','날인','자태','인제군','마지막',\n",
    "              '식목일','서귀포시','영등포구','성남시','고승민','발걸음','고범준','부제','치가','사거리','거리',\n",
    "              '롯데','주위','모습','월드','용산구','망신','이하늬','세영','야구','콜라','가드','광장',\n",
    "              '유혹','첫날','탐방','자락','세종시','맞이','패럴림픽','하루','이동면','경칩','성남','입춘',\n",
    "              '기승','보기','협약','기관','운영','수거','작업','조명','장병','주차장','올림픽','운항','빗방울',\n",
    "              '백승','곳곳','방역','하동','하동군','연휴','추석','황금','배병수','홍천','평창','과장',\n",
    "              '휴식','강릉시','유형','화창','동진','최진','사이','포럼','개최','조경규','국립',\n",
    "              '운세','후회','여기','직원','물든','대통령','호텔','바닥','북구','청와대','추연','유해',\n",
    "              '왼쪽','시장','공연','주제','오른쪽','수원시','촬영','주년','정원','청소','현지','여명','강남구',\n",
    "              '판매','통합','종합','네이버','지청','당분간','추상','햇살','연일','풍경','흥부가','위로',\n",
    "              '이틀','한경닷컴','유승민','무소속','조현아','노출','총선','스폰서','비키니','치어리더',\n",
    "              '수확','개방','상황','본부','관측','스탠드','당부','푸른','정상','김경','고현면','고현면',\n",
    "              '오색','정유년','저녁','서울대','훈련','채취','권현구','김인철','이정섭','미녀','몸짱','에버랜드',\n",
    "              '매일','서비스','최창호','피해자','병원','저작권','적발','사고','남남서','건물','신고','국제공항',\n",
    "              '교육청','행동','행동','데이트','확률','가끔','점차','전해','강원도','일이','핫이슈','봄꽃',\n",
    "              '오픈','서비스','윤성규','성매매','과천','대한항공','몸매','동영상','봉사활동','유강','정선',\n",
    "              '후보','문호','업데이트','고속도로','태양','물놀이','의상','쯔위','드레스','학교','현황',\n",
    "              '남남서','동해','내륙','중부지방','분야','폭스바겐','문화','광장','주연','벗꽃','리우','지사',\n",
    "              '경주','경주시','글로벌','실시간','인턴','후보','특별','형산강','연일읍','북한','마트','가동',\n",
    "              '한경닷컴','박기량','교육','인사','빗물','안양','그룹','농림축산식품부','이상인','대기',\n",
    "              '교수','내년','운동장','사막','내외','가장자리','여객선','호인','누드','확산','파문','자료',\n",
    "              '아나운서','산간','절기','만원','수도권','강수','기상대','주의','람사르','지리산','발전',\n",
    "              '결항','산수유','북주','외국인','인산인해','주상','부평','둘째','서울시장','지하철','공공기관',\n",
    "              '자자체','촉구','동남아','새끼','학원','산지','인파','썰매','사무소','휴일','로봇','금은',\n",
    "              '우리나라','적용','권현구','가정','연휴','나무','설경','시청','나흘','우수','터널','목련',\n",
    "              '일출','개나리','기운','스카이','연일','성동구','산불','진화','낙시터','평택','매장','대란',\n",
    "              '동아일보','공기','라며','산책','강종민','축구','프레','심기','작용','카메라','사랑','부처님',\n",
    "              '송골송골','이물질','개양귀비','구경','바다','시민활동','동급','튤립','해양','모내기','안개',\n",
    "              '경기장','둥지','지방선거','철쭉','탄천','드론','서귀포','비행','소만','추억','다리','고기압',\n",
    "              '대진','고송민','거부','활약','시민행동','선언','이의','취소','형형색색','무상','잠실','운행',\n",
    "              '대기오염','임현정','협약','전망대','한강','식용','군락','환경재단','어린이집','구름','울릉도',\n",
    "              '연꽃','밀양강','전문가','모두','코스모스','마포구','봄날','대박','중앙','배경','꽁꽁','단풍',\n",
    "              '눈길','기온','영하','분수','계곡','농장','화성','교양','가득','먼지','단계','고양','고양시',\n",
    "              '화단','이형기','알피','대학교','성큼','선선','참가자','태현','광진구','평창군','최동준',\n",
    "              '처서','동민','서툴','해운대해수욕장','해운대','극심','본격','물고기','니스','대관령','상공',\n",
    "              '전신','서해안','길이','나들','소재','시내','독도','동선','인공','산란','공개','창원시',\n",
    "              '민주당','이진욱','방문','노동','물이','삼척','구조','통행','밤사이','안산','뉴시스',\n",
    "              '서울','벚꽃','뉴스','시민','기자','주민','국민','지속','지자체','북적','불법','최대',\n",
    "              '두번째','세종로','질환','이용','보고','박원순','뿌연','유튜브','모든','주시 반영','주시',\n",
    "              '반영','아름','더위','기고','아름 다운','도심','다운','국적','고명진','어가','지고','지나',\n",
    "              '어진','이루','알리','상암동','대공','시작','연출','부터','면서','부터','유의','다가',\n",
    "              '면서','린다','만들기','마련','전시회','다소','거나','아지','나타','제거','총장','참석',\n",
    "              '발길','화려','라보','피하','가운','사흠','국적','매달','연출','대공','기고','반영','부터 아지',\n",
    "              '다가 부터','부터 부터','수요일','화요일','월요일','목요일','금요일','토요일','일요일','유의 바란',\n",
    "              '박원순','온라인','공식','신청','진행','활용','정도','설명','기간','이후','다른',\n",
    "              '발표','주장','지적','우려','가능성','정도','요구','과정','한편','동안','다시','상부',\n",
    "              '이어진','온라인','공식','우려','처음','이유','다시','정도','요구','크게','사실',\n",
    "              '기간','포함','다음','화보','한편','가능성','이후','설명','분석','지금','각각','발표',\n",
    "              '크게','동안','이후','처음','주요','내용','실시','어린이','확인','특보','비롯','분포',\n",
    "              '증가','중인','가량','아시아','이동','지역별','규칙','바로','이하'\n",
    "             ]\n",
    "            \n",
    "            stop_words = list(set(stop_words))\n",
    "\n",
    "            \n",
    "            if(len(s_words) != 0):\n",
    "                stop_words = s_words\n",
    "                \n",
    "            vect = CountVectorizer(stop_words = stop_words,ngram_range=(1, 2),min_df=0.005,max_df=0.3)\n",
    "       \n",
    "            X = vect.fit_transform(corpus)\n",
    "\n",
    "            count = X.toarray().sum(axis=0)\n",
    "            idx = np.argsort(-count)\n",
    "            count = count[idx]\n",
    "\n",
    "            feature_name = np.array(vect.get_feature_names())[idx]\n",
    " \n",
    "            co_word_list = list(zip(feature_name[:num], count[:num]))\n",
    "            co_word_list = pd.DataFrame(co_word_list)\n",
    "            filename = \"\".join(w1)\n",
    "            co_word_list.to_csv(\"./sentence_level_cowords/\" + prefix + filename + \"_scowords\" + \".csv\",encoding=\"UTF8\")\n",
    "            \n",
    "        def sentence_summarization(self,cnt=10):\n",
    "            \n",
    "            t_list = []\n",
    "            for s in self.filtered_sentences:\n",
    "                t= re.sub('[^가-힝0-9a-zA-Z\\\\s]', '', s)\n",
    "                t_list.append(t)\n",
    "                \n",
    "            p_sentences = \". \".join(t_list)\n",
    "            textrank = TextRank(b)\n",
    "            t = textrank.summarize(count=cnt)\n",
    "            t = a.split(\"\\n\")\n",
    "            \n",
    "            t = pd.DataFrame(t)\n",
    "            t.columns=['Sentences']\n",
    "            result.to_csv(\"./sentence_summary_result/\" + \"ss_\" + filename + \".csv\",encoding=\"UTF8\")\n",
    "        \n",
    "        # 명사 인식\n",
    "        def add_unknown_nouns(self):\n",
    "            file_list = self.filtered_idx_result_df['file_list'].tolist()\n",
    "            corpus = [] \n",
    "            for i in range(len(file_list)):\n",
    "                f = open(file_list[i], 'r',encoding='utf-8',errors=\"ignore\")\n",
    "                doc = f.read()\n",
    "                doc = doc.replace(\"\\n\",\"\")\n",
    "                corpus.append(\"\".join(doc))\n",
    "\n",
    "            corpus = pd.DataFrame(corpus)\n",
    "            corpus.to_csv(\"./corpus.txt\",sep=\" \", header=None)\n",
    "            \n",
    "            corpus_fname = './corpus.txt'\n",
    "            sents = DoublespaceLineCorpus(corpus_fname, iter_sent=True)\n",
    "\n",
    "            noun_extractor = LRNounExtractor_v2(verbose=True)\n",
    "            noun_extractor.train(sents)\n",
    "            nouns = noun_extractor.extract()\n",
    "            nouns = list(nouns.keys())\n",
    "            self.nouns = nouns\n",
    "            self.twitter.add_dictionary(nouns, 'Noun')\n",
    "                \n",
    "           # nouns\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "# 키워드 기반 기후변화 문서 정제\n",
    "\n",
    "na1 = NaverNewsAnalyzer()\n",
    "na1.filtering_date('2005-01-01','2017-12-31')\n",
    "na1.filtering_contents(na1.climate_words,False)\n",
    "\n",
    "na2 = NaverNewsAnalyzer()\n",
    "na2.filtering_date('2005-01-01','2017-12-31')\n",
    "# 토픽모델링을 통한 기후변화 클러스터 추출\n",
    "#na2.topic_modeling([5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],prefix = \"s1\",random_seed=1,ma=\"twitter2\")\n",
    "na2.load_lda_result(\"/s3_result_n_18_\")\n",
    "na2.select_topics([2,5,7,11,12])\n",
    "na = na1\n",
    "na.combine_idx_result(na2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
