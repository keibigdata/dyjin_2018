{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from os.path import basename   \n",
    "import os\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "class FileProcessor:\n",
    "    def __init__(self):\n",
    "        self.indexing = pd.DataFrame()\n",
    "        self.pdf_file_list = []\n",
    "        self.txt_file_list = []\n",
    "    \n",
    "    # DB로 부터 다운로드\n",
    "    def read_db_indexing_file(self):\n",
    "        self.indexing = pd.read_csv(\"./data/db_indexing_final.csv\",encoding='EUC-KR')\n",
    "        isNan = fc.indexing.iloc[:,2].isnull()\n",
    "    \n",
    "        self.indexing = self.indexing.fillna(\"NULL\")\n",
    "        \n",
    "        # .pdf찾기 (pdf 아닌것 제거)\n",
    "        f_list = list(self.indexing[\"SERVICE_URI\"])\n",
    "        is_pdf = [f.endswith('.pdf') for f in f_list ]\n",
    "        self.indexing = self.indexing.iloc[is_pdf,:]\n",
    "        \n",
    "        self.indexing = self.indexing.reset_index(drop=True)\n",
    "        \n",
    "    def download_KEI_reports(self):\n",
    "        for i in range(self.indexing.shape[0]):\n",
    "            #print(i)\n",
    "            url = self.indexing[\"SERVICE_URI\"][i]\n",
    "            #print(url)\n",
    "            new_path = \"./data/pdf/\" + os.path.basename(self.indexing[\"SERVICE_URI\"][i])\n",
    "\n",
    "            \n",
    "            # 파일이 있으면 다운로드 하지 않음\n",
    "            if os.path.isfile(new_path) == False :\n",
    "                urllib.request.urlretrieve(urllib.request.quote(url.encode('utf8'), '/:'),new_path)\n",
    "            self.pdf_file_list.append(new_path)\n",
    "        \n",
    "        pdf_file_list_df = pd.DataFrame(self.pdf_file_list, columns=[\"PDF_FILE\"])\n",
    "        self.indexing = pd.concat([self.indexing, pdf_file_list_df],axis=1)\n",
    "        \n",
    "        \n",
    "    def pdf_to_txt(self):      \n",
    "        save_dir = \"./result/txt/\"\n",
    "        for i in range(self.indexing.shape[0]):\n",
    "            new_file = save_dir + os.path.splitext(basename(self.pdf_file_list[i]))[0] + \".txt\"\n",
    "            if os.path.isfile(new_file) == False :\n",
    "                print(\"실행\")\n",
    "                cmd = \"pdftotext\" + \" \" + \"\\\"\" + self.pdf_file_list[i] + \"\\\"\" + \" \" + \"\\\"\" + new_file + \"\\\"\" \n",
    "                print(cmd)\n",
    "                os.system(cmd)\n",
    "            if os.path.isfile(new_file) == True :\n",
    "                self.txt_file_list.append(new_file)\n",
    "            else :\n",
    "                self.txt_file_list.append(np.NAN)\n",
    "                \n",
    "        txt_file_list_df = pd.DataFrame(self.txt_file_list,columns=[\"TXT_FILE\"])\n",
    "        self.indexing = pd.concat([self.indexing, txt_file_list_df],axis=1) \n",
    "        isNan = self.indexing.iloc[:,5].isnull()\n",
    "\n",
    "        self.indexing = self.indexing.dropna()\n",
    "        self.indexing = self.indexing.reset_index(drop=True)\n",
    "        print(self.indexing.columns)\n",
    "        #self.indexing.columns.appand(\"PDF_PATH\")\n",
    "        #self.indexing.columns.append(\"TXT_PATH\")\n",
    "        #print(self.indexing)\n",
    "        \n",
    "    def write_indexing_file(self):\n",
    "        self.indexing.to_csv(\"./result/indexing_final.csv\",header=True,index=None)\n",
    "   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/pdf/[95_정책]WO-04환경예산정책(곽태원).pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-adb8ada3fd58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFileProcessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_db_indexing_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload_KEI_reports\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mfc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpdf_to_txt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_indexing_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-788a0e3f70f5>\u001b[0m in \u001b[0;36mdownload_KEI_reports\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[1;31m# 파일이 있으면 다운로드 하지 않음\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquote\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'/:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpdf_file_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[1;31m# Handle temporary file setup.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0mtfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0mtfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNamedTemporaryFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/pdf/[95_정책]WO-04환경예산정책(곽태원).pdf'"
     ]
    }
   ],
   "source": [
    "fc = FileProcessor()\n",
    "fc.read_db_indexing_file()\n",
    "fc.download_KEI_reports()\n",
    "fc.pdf_to_txt()\n",
    "fc.write_indexing_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wed : Web Extraction from Docs\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from os.path import basename   \n",
    "import os\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "class WebSiteExtracter : \n",
    "    \n",
    "    def __init__ (self) :\n",
    "        self.indexing = pd.DataFrame()\n",
    "        self.website_list = []\n",
    "        self.website_df = pd.DataFrame()\n",
    "        self.category_df = None\n",
    "    def read_indexing_file(self):\n",
    "        self.indexing = pd.read_csv(\"./result/indexing.csv\")\n",
    "        self.indexing[\"KEI_RESEARCHAREA\"] = self.indexing[\"KEI_RESEARCHAREA\"].fillna(\"미분류\")\n",
    "        \n",
    "    def calculate_statistics(self):\n",
    "        category_cnt = Counter(self.indexing[\"KEI_RESEARCHAREA\"].tolist())\n",
    "        category_df = defaultdict(lambda: 0)\n",
    "        category_df['자원순환'] = category_cnt['자원순환관리'] + category_cnt['자원순환'] +  category_cnt['폐기물관리']\n",
    "        category_df['기후변화'] = category_cnt['기후변화'] + category_cnt['기후변화 대응'] +  category_cnt['기후변화대응']\n",
    "        category_df['자연환경'] = category_cnt['국토환경관리'] + category_cnt['자연환경'] \n",
    "        category_df['환경정책'] = category_cnt['환경정책'] + category_cnt['정책일반'] \n",
    "        category_df['환경영향평가'] = category_cnt['환경영향평가'] + category_cnt['환경평가'] \n",
    "        category_df['지구환경'] = category_cnt['지구환경'] + category_cnt['국제협력'] \n",
    "        category_df['대기환경'] = category_cnt['기후변화대기'] + category_cnt['대기환경'] \n",
    "        category_df['기타'] = category_cnt['기타'] + category_cnt['위해성관리'] + category_cnt['환경경제']\n",
    "        category_df['물환경'] = category_cnt['물환경']\n",
    "        category_df['환경보건'] = category_cnt['환경보건']\n",
    "        category_df['미분류'] = category_cnt['미분류']\n",
    "        self.category_df = category_df\n",
    "\n",
    "        \n",
    "    def extract_website(self):\n",
    "        website_list_cnt = defaultdict(lambda: 0)\n",
    "        category_list_cnt = defaultdict(lambda: 0)   \n",
    "        ko_keyword_list = defaultdict(lambda: \"\")\n",
    "        en_keyword_list = defaultdict(lambda: \"\")\n",
    "        \n",
    "        \n",
    "        categories = np.array(list(set(self.indexing[\"KEI_RESEARCHAREA\"])))\n",
    "        #print(categories)\n",
    "        \n",
    "        for i in range(self.indexing.shape[0]):\n",
    "            #print(file)\n",
    "            file = self.indexing['TXT_FILE'][i]\n",
    "            category = self.indexing['KEI_RESEARCHAREA'][i]\n",
    "            ko_keyword = self.indexing['KEI_KEYWORDKOREA'][i]\n",
    "            #print(ko_keyword)\n",
    "            en_keyword = self.indexing['KEI_KEYWORDENGLISH'][i]\n",
    "            #print(en_keyword)\n",
    "            \n",
    "            website_list = []\n",
    "            \n",
    "            #print(file)\n",
    "            f = open(file, 'r',encoding='utf-8',errors=\"ignore\")\n",
    "            doc = f.read()\n",
    "        \n",
    "            p = re.compile('([--:\\w?%&+~#=]*\\.[a-z]{2,4}\\/{0,2})((?:[?&](?:\\w+)=(?:\\w+))+|[--:\\w?@%&+~#=]+)?/g')\n",
    "            w_list = p.findall(doc)\n",
    "            if(len(w_list) == 0): next\n",
    "                \n",
    "            # 한문서 내에서는 1번만 사이트 카운트\n",
    "            w_list = list(set(w_list))\n",
    "            \n",
    "            # 제일앞에 아무것도 없는 경우는 http를 붙힘\n",
    "            for w in w_list:           \n",
    "                w = \"\".join(w)\n",
    "                if (w.startswith(\"http://\") == False and \n",
    "                    w.startswith(\"https://\") == False and \n",
    "                    w.startswith(\"ftp://\") == False):\n",
    "                    w = \"http://\" + w\n",
    "                #print(w)\n",
    "                website_list.append(w)     \n",
    "                \n",
    "            f.close()\n",
    "        \n",
    "            for w in website_list:\n",
    "                if (w.startswith(\"https://\")):\n",
    "                    fstr = \"http://\" + w[8:]\n",
    "                    #print(fstr)\n",
    "                    if fstr in website_list:\n",
    "                        website_list.remove(fstr)\n",
    "                        \n",
    "            for w in website_list:\n",
    "                try : \n",
    "                    website_list_cnt[w] += 1\n",
    "                    ko_keyword_list[w] += \",\" + ko_keyword\n",
    "                    en_keyword_list[w] += \",\" + en_keyword\n",
    "                    category_label = (categories == category)\n",
    "                    category_label = category_label.astype(int)\n",
    "                    category_label = category_label.tolist()\n",
    "                    category_list_cnt[w] =  (np.array(category_list_cnt[w]) + np.array(category_label)).tolist()                    \n",
    "                    \n",
    "                except : \n",
    "                    website_list_cnt[w] = 1 \n",
    "                    category_label = (categories == category)\n",
    "                    category_label = category_label.astype(int)\n",
    "                    category_label = category_label.tolist()\n",
    "                    category_list_cnt[w] = category_label\n",
    "\n",
    "        #print(ko_keyword_list)\n",
    "                    \n",
    "        # https주소에 대해서 http가 존재하면 http주소는 없애고 https에 카운트를 더함\n",
    "        \n",
    "        #print(website_list_cnt)\n",
    "        keys = list(website_list_cnt.keys())\n",
    "        for w in keys:\n",
    "            if (w.startswith(\"https://\")):\n",
    "                fstr = \"http://\" + w[8:]                \n",
    "                if fstr in keys:\n",
    "                    cnt = website_list_cnt[fstr]\n",
    "                    del website_list_cnt[fstr]\n",
    "                    category_cnt = category_list_cnt[fstr]\n",
    "                    del category_list_cnt[fstr]\n",
    "                    \n",
    "                    website_list_cnt[w] = website_list_cnt[w] + cnt\n",
    "                    category_list_cnt[w] =  (np.array(category_list_cnt[w]) + np.array(category_cnt)).tolist()\n",
    "                    try:\n",
    "                        ko_keyword = ko_keyword_list[fstr] \n",
    "                        del ko_keyword_list[fstr] \n",
    "                        ko_keyword_list[w] += \",\" + str(ko_keyword)\n",
    "                    except:\n",
    "                        ko_keyword_list[w] = \"\"\n",
    "                        \n",
    "                    try:\n",
    "                        en_keyword = en_keyword_list[fstr] \n",
    "                        del en_keyword_list[fstr] \n",
    "                        en_keyword_list[w] += \",\" + str(en_keyword)\n",
    "                    except:\n",
    "                        en_keyword_list[w] = \"\"\n",
    "                    \n",
    "        #print(ko_keyword_list)\n",
    "        website_df = pd.DataFrame(list(website_list_cnt.items()))    \n",
    "        category_df = pd.DataFrame(list(category_list_cnt.items()))\n",
    "        \n",
    "        colnames = [\"Website\",\"Cnt\",\"Website2\"]\n",
    "    \n",
    "        colnames = list(colnames) + list(categories)\n",
    "               \n",
    "        result_df = pd.DataFrame(columns = colnames)\n",
    "        \n",
    "        result_df[['Website','Cnt']] = website_df\n",
    "        result_df['Website2'] = category_df.iloc[:,0]\n",
    "         \n",
    "        temp = list(category_df.iloc[:,1])\n",
    "        temp = list(zip(*temp))\n",
    "        \n",
    "        for j in range(len(categories)):\n",
    "            result_df[categories[j]] = temp[j]\n",
    "                      \n",
    "        self.website_df = result_df\n",
    "        self.website_df.columns = colnames\n",
    "        self.website_df = self.website_df.sort_values(\"Cnt\",ascending=False)\n",
    "        self.website_df = self.website_df.reset_index()\n",
    "        \n",
    "        title_list = []\n",
    "        # add_title\n",
    "        # for i in range(website_df.count()[0]):\n",
    "        for i in range(10):\n",
    "            #print(i)\n",
    "            url = self.website_df[\"Website\"][i]\n",
    "            try :\n",
    "                cur_html = requests.get(url).text\n",
    "                soup = BeautifulSoup(cur_html, 'lxml')   \n",
    "                title = soup.select('head > title')\n",
    "                title = title[0].text\n",
    "                title_list.append(title)\n",
    "            except : \n",
    "                title_list.append(\"Error\")\n",
    "        title_list = pd.DataFrame(title_list,columns=[\"타이틀\"])\n",
    "        self.website_df = pd.concat([self.website_df ,title_list],axis=1)\n",
    "        \n",
    "        # 카테고리 제 분류\n",
    "        self.website_df['자원순환'] = self.website_df['자원순환관리'] + self.website_df['자원순환'] +  self.website_df['폐기물관리']\n",
    "        self.website_df['기후변화'] = self.website_df['기후변화'] + self.website_df['기후변화 대응'] +  self.website_df['기후변화대응']\n",
    "        self.website_df['자연환경'] = self.website_df['국토환경관리'] + self.website_df['자연환경'] \n",
    "        self.website_df['환경정책'] = self.website_df['환경정책'] + self.website_df['정책일반'] \n",
    "        self.website_df['환경영향평가'] = self.website_df['환경영향평가'] + self.website_df['환경평가'] \n",
    "        self.website_df['지구환경'] = self.website_df['지구환경'] + self.website_df['국제협력'] \n",
    "        self.website_df['대기환경'] = self.website_df['기후변화대기'] + self.website_df['대기환경'] \n",
    "        self.website_df['기타'] = self.website_df['위해성관리'] + self.website_df['환경경제'] \n",
    "        \n",
    "        self.website_df = self.website_df.drop('자원순환관리',axis=1)\n",
    "        self.website_df = self.website_df.drop('폐기물관리',axis=1)\n",
    "        self.website_df = self.website_df.drop('기후변화 대응',axis=1)\n",
    "        self.website_df = self.website_df.drop('기후변화대응',axis=1)\n",
    "        self.website_df = self.website_df.drop('국토환경관리',axis=1)\n",
    "        self.website_df = self.website_df.drop('정책일반',axis=1)\n",
    "        self.website_df = self.website_df.drop('환경평가',axis=1)\n",
    "        self.website_df = self.website_df.drop('국제협력',axis=1)\n",
    "        self.website_df = self.website_df.drop('기후변화대기',axis=1)\n",
    "        self.website_df = self.website_df.drop('위해성관리',axis=1)\n",
    "        self.website_df = self.website_df.drop('환경경제',axis=1)        \n",
    "        del self.website_df[\"Website2\"]\n",
    "        \n",
    "        ko_keyword_list.head(3)\n",
    "        en_keyword_list.head(3)\n",
    "        \n",
    "        self.website_df['KR_Keyword'] = ko_keyword_list\n",
    "        self.website_df['EN_Keyword'] = en_keyword_list\n",
    "        #print(self.website_df)\n",
    "        \n",
    "   \n",
    "        \n",
    "    def write_result(self):\n",
    "        self.website_df.to_csv(\"./result/result.csv\",index=None)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.defaultdict' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-6ca1f2e0defa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWebSiteExtracter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mwe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_indexing_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mwe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_website\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#plt.hist(we.indexing[\"KEI_RESEARCHAREA\"].tolist())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#we.calculate_statistics()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-2f37b5480f7c>\u001b[0m in \u001b[0;36mextract_website\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwebsite_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Website2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0mko_keyword_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0men_keyword_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.defaultdict' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "we = WebSiteExtracter()\n",
    "we.read_indexing_file()\n",
    "we.extract_website()\n",
    "#plt.hist(we.indexing[\"KEI_RESEARCHAREA\"].tolist())\n",
    "#we.calculate_statistics()\n",
    "#we.website_df\n",
    "#we.write_result()\n",
    "#we.indexing[\"category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "we.website_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
